% ============================================================================ %
% Encoding: UTF-8 (žluťoučký kůň úpěl ďábelšké ódy)
% ============================================================================ %

% ============================================================================ % 
\nn{Introduction}
Databases are an integral part of most systems and play a significant role in our lives. One of the most widely used database systems is Postgres, which was founded in the 1980s and continues to be the industry leader, employed in a wide range of applications. However, with the emergence of Kubernetes and the extensive migration of software systems to this orchestrator, the need to transition to Kubernetes for Postgres has arisen.

While Kubernetes is a versatile platform originally designed for deploying stateless microservice applications, it alone cannot fulfill all the requirements for achieving the high availability demanded on Postgres. To address this, CoreOS introduced the operator pattern, enabling the management of complex stateful applications that require synchronization between their nodes. An operator is a specialized software that extends the Kubernetes API and possesses specific knowledge of managing resources that Kubernetes lacks.

By utilizing the operator pattern, it becomes possible to achieve various functionalities for Postgres, such as high availability, backup to different cloud storage, Point-In-Time recovery, vertical or horizontal scaling, and upgrading to a new version of Postgres without human operator intervention. This eliminates the need for tedious and repetitive manual work, which is replaced by the Kubernetes Operator.

The focus of this thesis is to identify the appropriate Postgres lifecycle operators, establish metrics for creating a testing methodology, conduct tests, evaluate the results, and provide clear and comprehensive recommendations on selecting operators that align best with the defined metrics.

\n{1}{Thesis objective}
\label{chap:objectives}
The objective of this thesis is to conduct an evaluation of various Kubernetes operators available for Postgres lifecycle management.

The goal of the thesis is to deliver clear and comprehensive recommendations regarding the selection of operators that are best suited for managing the lifecycle of Postgres system based on defined metrics. This thesis intends to serve as a valuable resource that can guide stakeholders in making informed decisions about choosing the right operator for their specific operational context and requirements.

Furthermore, it aims to contribute to the broader knowledge base about Kubernetes operators and their application in managing Postgres databases in a cloud-native environment.



% ============================================================================ %
\cast{Theory}
\sloppy
\n{1}{Background}
This chapter introduces the key technologies used in this thesis including Postgres, Kubernetes, and Kubernetes operators.
\n{2}{Postgres}
PostgreSQL is a powerful object-relational database management system (ORDBMS) derived from the POSTGRES package written at the University of California at Berkeley. \cite{docuPgwhatIsPg} \cite{pg14introduction} The first version of POSTGRES was released in June 1989. POSTGRES has been used in many applications, including financial data analysis systems, asteroid tracking databases, medical information database, and several geographic information systems. The size of external community users has nearly doubled by 1993. \cite{docuPgBriefHistory}

POSTGRES was using its POSTQUEL query language from version, until Andrew Yu and Jolly Chen introduced SQL to POSTGRES in 1995. The name has changed to Postgres95. Postgres95 was completely ANSI C code reduced by 25 \% and was 30 – 50 \% faster than Postgres 4.2.  \cite{docuPgBriefHistory}

It was clear by 1996 that the name would not stand the test of time therefore it has been renamed to PostgreSQL. As stated by PostgreSQL documentation \cite{docuPgBriefHistory}: “Many people continue to refer to PostgreSQL as “Postgres” (now rarely in all capital letters) because of tradition or because it is easier to pronounce. This usage is widely accepted as a nickname or alias.“ This thesis will use Postgres as an alias for PostgreSQL as well.

More than 30 years after the first version Postgres has been considered the most used ORDBMS for professional developers by Stack Overflow survey \cite{so2022survey}. According to Riggs and Ciolli \cite{pg14introduction}: “The PostgreSQL feature set attracts serious users who have serious applications. Financial services companies may be PostgreSQL's largest user group, although governments, telecommunication companies, and many other segments are strong users as well.“ It is fully ACID compliant \cite{juba2015learningTransactionIsolation}
and supports many kinds of data models such as relational, document, and key/value. \cite{pg14introduction} The Postgres architecture can be observed in Figure \ref{fig:Postgres_architecture}

\obr{Postgres Architecture \cite{HusseinMediumPostgres} }{fig:Postgres_architecture}{1}{graphics/Postgres_architecture.png}


\n{3}{Write Ahead Log}
Write-ahead Logging (WAL) is esential Postgres technique to ensure data integrity. Its main concept is that changes in data files (where tables and indexes are stored) must only be written after they are logged (saved to a log file). That means the database is updated after the changes are written to disk. In the event of a system crash, all transactions will be recovered from the disk. \cite{docuPgWal}

Although WAL is primarily designed for recovery after a database server crash, its design also allows any changes to the database server state to be replayed backward. A copy of the log is also a form of backup. Thus, for recovery to a point in time, only logs that have been saved to that point in time can be restored. This technique is called Point-In-Time Recovery (PITR). \cite{DocuPgPITR} These log files can also be streamed to other nodes to serve as a replica or remote backup. \cite{pg14replication}

\n{3}{Backup and restore}
A full set of backup commands is included in Postgres. Among the simple backup commands are pg\_dump and pg\_dumpall, which enable one or more databases to be saved in SQL format. A wide range of configuration options are available for these commands, including compression for large databases or exporting only the database schema. To restore a database from a file at a later time, the psql command can be used, which is capable of restoring a database from its dump. \cite{DocuPgDump} These commands are also helpful with migration from one major Postges version to another because the dumped files are plain SQL commands.

However the backup options in Postgres are quite limited. Postgres allows to set up of a backup command that runs after the next log file is created, database dumps, and log streaming. For more advanced backup techniques, like scheduled backup or cloud backup, additional software such as PgBackRest must be utilized. \cite{DocuPgPITR}

\n{4}{PgBackRest}
PgBackRest is a reliable and simple backup and restore solution that provides many features on top of classic Postgres backup and restore tools like parallel backup options with compression, local or remote backups, cloud backup (S3. Azure and Google Cloud), or backup encryption. Full, incremental, or differential backup is also supported. \cite{PGbackRest}

\n{3}{High Availability}
The basic structure of a database cluster consists of one or more database servers, which can be called nodes. In Postgres there are two types of nodes, Primary node and Standby node.  A Primary node is such a node that allows reading and writing information. The newly written information is then streamed to the Standby nodes. Standby nodes are read-only, they do not allow writing. \cite{pg14replication}

Achieving high availability with Postgres is possible by using more than one node in the cluster. Two options are possible here. A single Primary node option, where the Primary node is read and write enabled, and the other nodes are Standby nodes. If the Primary node is unavailable, then the Standby node is promoted to the Primary node. If this event is planned it is called a switchover event, otherwise it is a failover. In this variant, the Primary node streams the logs to the Standby nodes. The second option is to use multiple Primary nodes. However, conflicts can occur because all Primary nodes allow concurrent writes. \cite{docuPgHA}
\n{4}{Patroni}
Since Postgres does not provide any software that can detect that a node is unavailable, it is necessary to use software outside of Postgres \cite{docuPgFailover}, such as Patroni.
Patroni is a popular open-source tool created by Zalando to achieve high availability of Postgres clusters. Patroni uses a distributed configuration source such as ZooKeeper, Etcd, Consul, or Kubernetes for its operation. Patroni can automatically adjust the settings of all managed nodes, therefore it can automate failover and make it seamless. \cite{PalarkMigratingPg} \cite{PatroniDocu}


\n{3}{Load Balancing and Connection Pooling}
Using more than one node allows to direct traffic to a node that is less busy and thus achieve load balancing. Postgres doesn't come with any software that allows splitting the load on different nodes, so it is necessary to use an external load balancer such as HA Proxy or pgBouncer. The load balancer then acts as an intermediary between the database and the client and directs the traffic to the available nodes according to the set rules. These load balancers also enable connection pooling which is a technique for managing and reusing database connections to increase performance and reduce overhead. Connection pooling involves creating a pool of pre-created connections that can be shared and reused by multiple client requests, instead of creating a new connection for each request. This removes the overhead of creating a new process each time a client connects to Postgres and allows the client to use resources that would otherwise be used to service multiple requests (or complete them faster). \cite{PerconaBlogConnectionPooling}

\n{2}{Kubernetes}
\label{chap:kubernetes}
Kubernetes, also known as K8s, is an open-source platform for automating deployment, scaling, and management of containerized applications. It provides a way to manage and orchestrate containers, which are units of software that package up an application and its dependencies into a single, isolated package that can run consistently on any infrastructure. \cite{vayghan2019Kubernetes}

As described by Kubernetes Documentation \cite{docuKubeComponents} Kubernetes provides several key features, including:
\begin{itemize}
    \item \textbf{Service discovery:} A container can be exposed by Kubernetes either through its DNS name or its own IP address.
    \item \textbf{Load balancing:} In the case of high traffic to a container, stability of the deployment can be ensured by Kubernetes load balancing and distributing the network traffic.
    \item \textbf{Storage Orchestration:} Storage orchestration in Kubernetes allows for the automatic mounting of a storage system of choice, including local storage, public cloud providers, and others.
    \item \textbf{Automated rollouts and rollbacks:} The desired state of deployed containers can be described using Kubernetes, and the actual state can be changed to the desired state at a controlled rate. For instance, the automation of Kubernetes can be utilized to create new containers for the deployment, remove existing containers, and transfer all their resources to the newly created container.
    \item \textbf{Automatic bin packing:} A cluster of nodes for running containerized tasks is provided to Kubernetes. The amount of CPU and memory required by each container is specified to Kubernetes. The optimal utilization of resources can be achieved by Kubernetes fitting the containers onto the nodes.
    \item \textbf{Self healing:} Containers that fail are restarted by Kubernetes, those that do not respond to the user-defined health check are replaced or killed, and they are not advertised to clients until they are deemed ready to serve.
    \item \textbf{Secret and configuration management:} Sensitive information, such as passwords, OAuth tokens, and SSH keys, can be stored and managed by Kubernetes. The deployment and updating of secrets and application configuration can be done without the need to rebuild container images and without the exposure of secrets in the stack configuration.
\end{itemize}
\n{3}{Kubernetes Components}
\label{chap:kubeComponents}
Kubernetes cluster is composed of a set of worker machines that run containerized applications called nodes. Each cluster must have at least one node. \cite{docuKubeComponents}
\obr{The components of a Kubernetes cluster \cite{docuKubeComponents}}{fig:kubeComponents}{1}{graphics/Kubernetes_cluster_components.png}

The Kubernetes control plane is the management system of a Kubernetes cluster, responsible for maintaining the desired state of the cluster.
As depicted in Figure \ref{fig:Postgres_architecture} it consists of multiple components that work together to manage the cluster and its resources, including pods, services, and volumes. The key components of control plane are \cite{masteringKubernetesConcepts}:
\begin{itemize}
    \item \textbf{kube-APIserver:} Acts as the front-end for the Kubernetes API and exposes the API to other components. \cite{docuKubeComponents}
    \item \textbf{Etcd:} Highly available distributed key-value store that serves as the backing store for the cluster's configuration data. \cite{Dobies2020}
    \item \textbf{kube-scheduler:} Assigns work to nodes in the cluster, such as scheduling pods to run on nodes. \cite{kubeUpAndRunningPods}
    \item \textbf{kube-controller-manager:} Monitors the cluster's state and makes adjustments as necessary to maintain the desired state. \cite{masteringKubernetesConcepts}
    \item \textbf{cloud-controller-manager:} Manages cloud-related tasks such as node creation and management, volume management, and load balancing, allowing the other components of the control plane to focus on their specific responsibilities. Cloud manager is optional. Can be avoided when Kubernetes not used in cloud. \cite{docuKubeComponents}
\end{itemize}
\textbf{Node components:}
Node components in a Kubernetes cluster run on each node and provide crucial functionality for the operation of containers on that node. \cite{docuKubeComponents}
\begin{itemize}
    \item \textbf{kubelet:} Is responsible for communicating with the control plane and ensuring that containers are running and healthy. \cite{kubeUpAndRunning}
    \item \textbf{kube-proxy:} Is responsible for maintaining network rules on the nodes, allowing network communication to the containers. It enables the containers in a pod to communicate with other containers and the outside world, and performs tasks such as load balancing and traffic routing. \cite{kubeUpAndRunning}
    \item \textbf{container runtime:} Is responsible for running containers. \cite{docuKubeComponents}
\end{itemize}

\n{3}{Kubernetes Concepts}
\label{chap:kubeConcepts}

Pod is the smallest deployable unit that can be created in Kubernetes. \cite{docuKubePods} A Pod in Kubernetes is comprised of multiple containers and storage volumes that are run together within the same execution environment. As a result, all containers included in a single Pod will always run on the same machine. \cite{kubeUpAndRunningPods}
A Pod's specifications are outlined in a Pod manifest, which is simply a JSON or YAML text file that represents the Kubernetes API object. Kubernetes follows a declarative configuration approach, where the system's desired state is defined in a configuration file, and the service then implements the necessary changes to make the desired state a reality. \cite{docuKubeStaticPod}

ReplicaSet's purpose is to ensure a consistent number of replica Pods are running at all times. It is commonly used to guarantee a specified number of identical Pods are available. However, a Deployment is a more advanced concept that oversees ReplicaSets and provides a more streamlined way to make updates to Pods. It also offers additional features. As a result, it's advisable to use Deployments instead of directly utilizing ReplicaSets, unless there is a need for specific update requirements or no need for updates at all. \cite{docuKubeReplicaset}

Service is an abstraction layer and defines a group of Pods and the method to access them (often referred to as a micro-service). The group of Pods targeted by a Service is usually specified through a selector. The Service abstraction makes this possible by enabling the decoupling of components. \cite{docuKubeSevice} Kubernetes includes built-in service discovery mechanisms. When a service is created in Kubernetes, it is automatically assigned an IP address and DNS name. Clients and other services can use this name or address to access the service within the Kubernetes cluster. \cite{docuKubeSevice}

Containers and pods in Kubernetes are ephemeral. When a container is terminated, any data it has written to its own filesystem is lost. In Kubernetes, storage is represented by a basic abstraction called "volumes". Containers use these volumes by binding them to their respective pods, and can then access the storage regardless of its physical location as if it were a part of their local filesystem. \cite{masteringKubernetesStorage}

Kubernetes version 1.5 came with a new object called StatefulSet that allows a set of stateful pods to be deployed and managed. Each pod has a unique, stable network identity and a persistent storage volume. This enables stateful applications like databases to be run on Kubernetes. Advantages of using StatefulSets include predictable naming schemes, ordered pod creation and deletion, and unique persistent storage. \cite{docuKubeStatefulSet} \cite{githubKube15}

In version 1.7, Kubernetes introduced the Custom Resources extension to its API. \cite{githubIBMCr} This extension allows Kubernetes to use user-defined resources that are not native to Kubernetes as if they were native. \cite{NewStackCRDs} Custom resources (CR) is an extension to the Kubernetes API that extends the deployment with additional parameters that are not part of it. CR stores these parameters and allows the API server to access them just like the native Kubernetes parts. CR is created in the Kubernetes cluster using a definition called Custom Resource Definition (CRD). \cite{operatorsAtK8sIface}

Kuberentes controllers (depicted in Figure \Ref{fig:kubeControler}) are control loops\footnote[1]{A control loop is a process that continuously monitors the state of a system, compares it to a desired state, and makes adjustments to bring the system closer to the desired state.} that constantly check the state of their controlled objects. If the controlled objects are not in the desired state, the controller performs actions to get the controlled objects into that state. For example, restart a crashed node, add a new replica, modify settings, etc. \cite{docuKubeControllers}
However, to work with CR, custom controllers that can work with these resources must be created, these controllers are called Custom Controllers. \cite{docuKubeCR}


\obr{Kubernetes controller \cite{operatorWhitepaper} }{fig:kubeControler}{1}{graphics/Kubernetes_controller.png}

\n{2}{Running Postgres in Kubernetes}
\label{chap:postgresInKube}
Kubernetes cannot know all complex stateful applications, which can contain a large number of nodes and have a wide range of uses while remaining general-purpose.
The goal of Kubernetes is to provide an abstraction covering basic application concepts and providing options for extensions for more complex applications and their specific operations.
Kubernetes cannot and should not know all the possible settings and operations that, for example, a Postgres cluster needs to run. \cite{operatorsTeaches}

The easiest way to run Postgres in Kubernetes is through the StatefulSet mentioned in Chapter \ref{chap:kubeComponents}. This StatefulSet can start a Postgres pod, create a persistent volume, and connect this volume to the pod. A stateful set can do this for all replicas set in its configuration. It can also scale up or down. Unfortunately, however, all independent Postgres instances created by StatefulSet controller are not synchronized in any manner.

While this simple approach may be sufficient for running a single node, it is no longer sufficient for managing the whole Postgres lifecycle.
For managing whole Postgres lifecycle it is necessary to install additional applications in the Kubernetes cluster and then configure the Postgres to work with them. This represents a large amount of work and subsequent maintenance that Kubernetes operators can facilitate.

\n{2}{Database system lifecycle}
\label{chap:lifecycle}
The database system itself is a software like any other. It is therefore also subject to the same lifecycle as software.
As depicted in Figure \ref{fig:applifecycle} application lifecycle consists of three main parts. It is the governance part, development, and operations.
For this thesis, the focus will be specifically on the operations part, as it is the only controllable aspect considered for testing and evaluation.
\obr{Application Lifecycle \cite{ALM}}{fig:applifecycle}{1}{graphics/aplication-lifecycle.png}

Operation is the process of running and managing the application, which starts with deployment and continues until the application is taken out of service. This aspect of the application lifecycle management covers the release of the application into production, ongoing monitoring, and other related tasks. \cite{ALM}

Therefore the complete database system lifecycle can be outlined by following events:
\begin{itemize}
    \item System installation
    \item System upgrade to a newer version (major and minor)
    \item System backup
    \item System restore
    \item System monitoring
    \item System scaling (vertical and horizontal)
    \item System configuration update
    \item System uninstall
\end{itemize}

\pagebreak
\n{2}{Operators}
\label{chap:operators}
Kubernetes can run stateless applications very well. But its general purpose makes running complex stateful applications on top of it quite challenging.

However, this has changed in 2016 when CoreOS came up with operators (depicted in Figure \Ref{fig:operatorDefinition}) as a way to deploy complex applications with state such as databases, caches, or monitoring systems. \cite{IArchiveCOSoperators}

An operator is a special kind of software that extends the Kubernetes API and has a particular knowledge of managed resource that Kubernetes does not have.
The operator also serves as a packaging mechanism for distributing applications including their dependencies in Kubernetes. The operator can manage, restore, update or monitor the resource. It can also manage very complex applications. The Kubernetes operator thus replaces the human operator after which it is named, who would otherwise take care of these tasks. \cite{operatorsPreface} \cite{IArchiveCOSoperators}

\obr{Definition of Kubernetes operator \cite{IArchiveCOSoperators}}{fig:operatorDefinition}{1}{graphics/coreos_operator.png}

CoreOS demonstrated the use of its operator on Etcd (described in the Kubernetes Components chapter). When new Etcd nodes are created, it is necessary to give them a DNS names and use the Etcd cluster management tools to add the new nodes to an existing cluster. CoreOS has automated these tasks with the Etcd operator so that all that is required is to increase the number of replicas in the operator CRD and the Etcd operator will perform these tasks instead of a human operator. \cite{IArchiveCOSoperators}
By embedding the human operator's operational knowledge into the code, this ensures that these tasks are repeatable, testable and upgradable. It also ensures that the necessary operations are always performed, executed in the order in which they are supposed to be performed, and none are skipped. This reduces the number of hours spent on dull but essential work such as backups. \cite{operatorWhitepaper}

As described by operator White Paper \cite{operatorWhitepaper} and depicted in Figure \ref{fig:operatorPatern}, operator consists of the following parts
\begin{itemize}
    \item The managed application or infrastructure
    \item Software that has some specific knowledge of the managed application or infrastructure and allows the user to declaratively set the desired state
    \item Custom Controller, which is responsible for achieving the desired state
\end{itemize}

\obr{Operator pattern \cite{operatorWhitepaper} }{fig:operatorPatern}{1}{graphics/operator_patern.png}

Like human operators, Kubernetes operators can have a level of manual skill ranging from basic software installation and setup skills to a high level where they can scale software vertically or horizontally to automatically change the configuration or detect abnormalities. All operator maturity levels are depicted in the Figure \ref{fig:operatorCapabilities}. The highest level can only be reached by programming the operator in the GO programming language or by using the Ansible automation tool. \cite{operatorsOframework}

\obr{Operator maturity levels described by Operator Framework \cite{OFrameworkMaturity}}{fig:operatorCapabilities}{1}{graphics/operator_capabilities.png}

As stated in the Operator white paper, \cite{operatorWhitepaper} the operator should be able to cover the complete lifecycle of the managed resource as defined in the previous chapter without the need for external installation or upgrade intervention. Specifically as follows:
\begin{itemize}
    \item Install or take ownership of the controlled application.
    \item Upgrade the managed application, including the monitoring of the upgrade process. It should also be able to roll back in case of failure.
    \item Back up the managed application and log when the application was last backed up and the status of that backup.
    \item Restore the application from the backup.
    \item Provide monitoring of the managed application.
    \item Scale the application.
    \item Automatically adapt the configuration of the application.
    \item Uninstall or disconnect from the application.
\end{itemize}

These are all capabilities that an operator should have at the highest level No. 5 - Autopilot. For lifecycle management described in Chapter \ref{chap:lifecycle}, the minimum level of operator capabilities must be at least level No. 4 - Deep Insights with an option to scale.

The Kubernetes cluster is divided into individual namespaces that separate the objects and names in the cluster and can have constraints applied to them. This partitioning makes it easier to share the cluster between users or entire teams. The object name must be unique within a namespace, but not between namespaces.  An operator usually operates in its own namespace so it has a Namespace Scope, but it can also operate in the whole cluster in which case it will be a Cluster Scope operator. Namespace Scope operators are more flexible and easier to upgrade due to their independence from the rest of the cluster. Operator rights are further restricted by the so-called Role-Based Acceess Control (RBAC), which grants the rights assigned to the operator. \cite{ operatorsAtK8sIface}

The following options are advised by the Operator white paper \cite{operatorWhitepaper} in case the operator is to be used for managing the resource:
\begin{itemize}
    \item	Consultation with the creator of the resource to be controlled about the possibilities of using the operator.
    \item	The search for public operator registries that provide a platform for publishing operators and the underlying documentation.
    \item	The creation of own operator.
\end{itemize}

\n{1}{Resource questions}
\label{chap:resourceQuestions}
With the thesis objective defined and a comprehensive understanding of Postgres, Kubernetes, the Postgres lifecycle, and operators established in previous chapters, the research questions can now be formulated.
These questions are aimed at facilitating a deeper exploration of the intricacies involved in managing Postgres in a Kubernetes environment through the use of operators.

\begin{enumerate}
    \item What operators exist for lifecycle management of Postgres in Kubernetes?
    \item What metrics are suitable for comparing Opereators for lifecycle management in Kubernetes?
    \item What approach should be taken to determine the degree to which the metrics are met?
    \item How do the operators perform when evaluated according to the chosen metrics?
\end{enumerate}

\n{1}{Operators for Lifecycle Management in Kubernetes}
\label{chap:searchForoperators}
This chapter aims to answer the first research question: 'What operators exist for lifecycle management of Postgres in Kubernetes?'"
As recommended in chapter~\ref{chap:operators} the selection of the operator should first be consulted with the manufacturer of the controlled source. Postgres offers the following Kubernetes operators in its software catalog \cite{docuPgSwCatalogue}: CloudNativePG, EDB Postgres for Kubernetes a Kubegres.

The next recommended step is to search operator registries. In particular the Operator Hub. \cite{operatorHubPGSearch} Operator Hub presents eight operators with varying levels of capabilities, including Crunchy Postgres for Kubernetes by Crunchy Data, EDB Postgres for Kubernetes by EnterpriseDB Corporation, Ext Postgres Operator by movetokube.com, Percona Operator for PostgreSQL by Percona, Postgres-Operator by Zalando SE, Postgresql Operator by Openlabs, PostgreSQL Operator by Dev4Ddevs.com and StackGres by OnGres.

By further research the Stolon operator was revealed. \cite{PalarkComparingKubernetes}

Of the twelve operators available, only five meet the minimum capability requirement of Deep Insight with a possibility to scale defined in Chapter \ref{chap:operators}, namely: Crunchy Postgres for Kubernetes, EDB Postgres for Kubernetes, Percona operator for PostgreSQL, CloudNativePG operator, and StackGres operator. As a result, only these five will be subjected to deeper research, testing, and evaluation.

\pagebreak
\n{2}{Crunchy Postgres for Kubernetes}
Crunchy Postgres for Kubernetes (PGO) is a Postgres operator provided by Crunchy Data, which offers a declarative solution for the management of PostgreSQL clusters, with a focus on automation.
Crunchy Data is a company that specializes in providing open-source software solutions for Postgres. The company also provides a range of support, consulting, and training services to help organizations implement and optimize their Postgres deployment. \cite{Crunchy}

PGO’s capabilities are the following:
\begin{itemize}
    \item \textbf{Postgres Cluster Provisioning:} PGO is able to create \cite{CrunchyDocCreate}, update \cite{CrunchyDocUpdate} or delete Postgres cluster \cite{CrunchyDocDelete}
    \item \textbf{High Availability:} High availability is achieved by adding additional nodes. PGO uses a synchronous replication technique with Primary and Standby architecture. \cite{CrunchyDocHA}
    \item \textbf{Postgres updates:} PGO is able to apply minor patches \cite{CrunchyDocMinorUpdates}, and major upgrades since version 5.1. \cite{CrunchyBlogUpdates}
    \item \textbf{Backups:} PGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob), ad hoc backups, backup compression, and backup encryption. \cite{CrunchyDocBackups}
    \item \textbf{Disaster Recovery:} PGO is capable of Point-In-Time recovery, in place Point-In-Time Recovery, restore of an individual database. \cite{CrunchyDocDisasterRecovery}
    \item \textbf{Cloning:} PGO is able to clone cluster. \cite{CrunchyDocDisasterRecovery}
    \item \textbf{Monitoring:} Monitoring is provided by Prometheus, Grafana, and Alertmanager. \cite{CrunchyDocMonitoring}
    \item \textbf{Connection Pooling:} PgBouncer connection pooler is part of PGO. \cite{CrunchyDocConnectionPooling}
    \item  \textbf{Customization:} PGO provides a wide area of Postgres customization. \cite{CrunchyDocCustomisation}
\end{itemize}


PGO consists of the following key components \cite{CrunchyPGOGit}:
\begin{itemize}
    \item High Availability: Patroni
    \item Backups: PgBackRest
    \item Connection Pooler: PgBouncer
    \item Monitoring: PgMonitor, Prometheus, Grafana, and Alertmanager
\end{itemize}
\obr{PGO’s architecture \cite{CrunchyDocArchitecture}}{}{1}{graphics/pgo_architecture.png}

The current stable version of PGO is 5.3.1 was released on 17th February 2023. \cite{CrunchyV531releaseNotes}

PGO is distributed under the Apache License 2.0, an open-source license that allows for both commercial and non-commercial use. With regards to capability, PGO is considered to have the highest capability level, labeled as Autopilot. \cite{operatorHubCrunchy}

\n{2}{EDB Postgres for Kubernetes}
The EDB Postgres for Kubernetes (EDBO) is a operator that has been designed, developed, and maintained by EnterpriseDB Corporation.
It provides comprehensive coverage of the entire lifecycle of highly available Postgres database clusters with a Primary/Standby architecture,
utilizing native streaming replication. The operator is based on the open-source CloudNativePG operator and offers additional benefits. \cite{operatorHubEDB}

EDBO is distributed under the EDB Limited Usage License Agreement, a proprietary license that is specific to software provided by EnterpriseDB Corporation. A license key is always required for the operator to work longer than 30 days. \cite{EDBdocuLicence} Due to the restrictive nature of the license EDBO will no longer be subject to testing and evaluation but its key component CloudNativePG will.

\n{2}{CloudNativePG}
The CloudNativePG operator (CNPGO) is an operator that is available as an open-source solution and aims to manage Postgres workloads across various Kubernetes clusters running in private, public, hybrid, or multi-cloud environments. The operator aligns with DevOps principles and concepts like immutable infrastructure and declarative configuration. \cite{CNPGdocu}

Initially developed by EDB, CNPGO was later made available to the public as an open-source software under the Apache License 2.0. In April 2022, the project was submitted to CNCF Sandbox for further development and community engagement. \cite{CNPGdocu}

CNPGO’s capabilities are the following:
\begin{itemize}
    \item \textbf{Postgres Cluster Provisioning:} CNPGO is able to create, update or delete Postgres cluster. \cite{CNPGdocuCapabilityLevels}
    \item \textbf{High Availability:} High availability is achieved by adding additional nodes. PGO uses a synchronous replication technique with Primary and Standby architecture. \cite{CNPGdocuReplication}
    \item \textbf{Direct database imports:} CNPGO provides direct database import from remote Postgres server by using pg\_dump and pg\_restore even on different Postgres versions. \cite{CNPGdocuDatabaseImports}
    \item \textbf{Postgres updates:} CNPGO is able to apply minor patches. \cite{CNPGdocuUpdates} Major updates are possible by Direct database imports\footnote[2]{Due to its nature Direct database imports cannot be considered as major upgrade option.}.
    \item \textbf{Backups:} CNPGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob), on-demand backups, and backup encryption \cite{CNPGdocuBackup}\cite{CNPGdocuTDE}.
    \item \textbf{Disaster Recovery:} CNPGO is capable of Point-In-Time recovery. \cite{CNPGdocuBackup}
    \item \textbf{Cloning:} CNPGO is able to create cluster replicas. \cite{CNPGdocuReplication}
    \item \textbf{Monitoring:} Monitoring can be provided by the additional installation of Prometheus, and Grafana, and Alertmanager. \cite{CNPGdocuQuickstart}
    \item \textbf{Connection Pooling:} Provided by native Postgres pooler PgBouncer. \cite{CNPGdocuConnectionPooling}
    \item \textbf{Customization:} CNPGO provides a wide area of Postgres customization such as max parallel workers tuning or WAL configuration \cite{CNPGdocuConfiguration}
\end{itemize}

CNPGO consists of the following key components \cite{PostgresOnKubernetes} \cite{CNPGdocuQuickstart}:
\begin{itemize}
    \item High Availability: Postgres instance manager
    \item Backups: Barman
    \item Connection Pooler: PgBouncer
    \item Monitoring: Prometheus, Grafana, and Alertmanager
\end{itemize}

\obr{CNPGO’s architecture \cite{CNPGdocuConnectionPooling}}{}{1}{graphics/CNPGO_architecture.png}

The current major stable version of CNPGO is 1.20.0 was released on 27th April 2023. \cite{CNPGreleases} CNPGO is distributed under the Apache License 2.0 open-source license. CNPGO is considered to have the highest capability level, labeled as Autopilot. \cite{CNPGdocu}

\n{2}{StackGres operator}
StackGres (SPGO) is a comprehensive distribution of Postgres for Kubernetes, delivered in a user-friendly deployment package. The distribution includes a set of Postgres components that have been carefully selected and optimized to work seamlessly with each other. \cite{SPGOgitlab}

SPGO is developed by OnGres that was established as a result of years of experience in working with and creating products based on Postgres and supporting clients with their Postgres infrastructures. Postgres databases are at the heart of the company's business, as the name suggests. \cite{OnGres}

SPGO’s capabilities are the following \cite{OnGres}:
\begin{itemize}
    \item \textbf{Postgres Cluster Provisioning:} SPGO is able to create, update or delete Postgres cluster.
    \item \textbf{High Availability:} High availability is achieved by adding additional nodes with Primary and Standby architecture.
    \item \textbf{Postgres updates:} SPGO is able to apply minor patches. Major updates are possible by SGDbOps \cite{SPGODocuMajorUpdates}.
    \item \textbf{Backups:} SPGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob)
    \item \textbf{Disaster Recovery:} SPGO is capable of Point-In-Time recovery.
    \item \textbf{Cloning:} SPGO is able to create cluster replicas.
    \item \textbf{Monitoring:} Monitoring is provided by Prometheus, Grafana, and Alertmanager.
    \item \textbf{Connection Pooling:} Is provided by PgBouncer.
    \item \textbf{Customization:} SPGO provides a wide area of Postgres customization such as WAL configuration, archive mode, vacuum, etc. \cite{SPGODocuCustomization}
    \item \textbf{Mamagement Console:} SPGO provides a fully featured management web console.

\end{itemize}

SPGO consists of the following key components \cite{PostgresOnKubernetes}:
\begin{itemize}
    \item High Availability: Patroni
    \item Backups: WAL-G
    \item Connection Pooler: PgBouncer
    \item Monitoring: Prometheus, Grafana, and Alertmanager.
\end{itemize}

\obr{SPGO’s architecture \cite{SPGOdocuArchitecture}}{}{1}{graphics/SPGO_architecture.png}

The current stable version of SPGO is 1.4.3\footnote[3]{Version 1.5.0 has also been released, it is not production-ready yet. Therefore will not be tested or evaluated.} was released on 20th February 2022. \cite{SPGOgitlabChangelog} SPGO is distributed under the AGPL3 open-source license. \cite{SPGODocuLicence}
With regards to capability, SPGO is considered to have the second highest capability level, labeled as Deep Insights. \cite{operatorHubStackgres}

\n{2}{Percona operator for PostgreSQL}
\label{chap:ppo}

Percona is a company that provides services and solutions for open-source database technologies. It offers expertise, support, and software for MySQL, MongoDB, and PostgreSQL. The company's offerings help organizations manage their open-source databases and ensure optimal performance, security, and scalability. \cite{Percona}

Percona operator for PostgreSQL (PPO) is based on Crunchy Postgres for Kubernetes. Percona forked PGO v 4.7 and has added enhancements for monitoring, upgradability, and flexibility. \cite{PerconaBlogProsAndCons}

Differences between PGO and PPO are the following:
\begin{itemize}
    \item \textbf{Postgres updates:} PPO provides automatic Postgres updates for minor and major versions of Postgres.  \cite{PerconaDocuUpdate}
    \item \textbf{Backups:} PPO is not able to back up to Azure. \cite{PerconaDocuCompare} Although it uses Patroni, which has this ability.
    \item \textbf{Disaster Recovery:} PPO documentation does not mention the possibility of restoring a single database from a backup. \cite{PerconaDocuBackups}
    \item \textbf{Monitoring:} PPO is not using the usual monitoring stack consisting of Prometheus and Grafana but their own Percona Monitoring and Management. \cite{PerconaDocuMonitoring}
\end{itemize}

The current stable version of PPO is 1.4.0 was released on 31st March 2023\footnote[4]{Version 2.0.0 has also been released, it is not production-ready yet. Therefore will not be tested or evaluated.}. \cite{PerconaDocuV2} PPO is distributed under the Apache License 2.0, an open-source license that allows for both commercial and non-commercial use. With regards to capability, PPO is considered to have the second highest capability level, labeled as Deep Insights. \cite{operatorHubPercona}

\n{2}{Summary and key differences}
\tab{Summary of selected operators}{tab:operatorsSummary}{1}{|l|l|l|l|}{
    \hline operator & Maturity level & Current production version & Release date\\ \hline
    PGO & Autopilot & 5.3.1 & 17th February 2023\\ \hline
    CNPGO & Autopilot & 1.20.0 & 27th April 2023\\ \hline
    SPGO & Deep Insights & 1.4.3 & 20th February 2022\\ \hline
    PPO & Deep Insights & 1.4.0 & 31st March 2023\\ \hline
}

\tab{Key differences between selected operators}{tab:operatorsDiffences}{1}{|l|l|l|l|l|}{
    \hline Feature & PGO & CNPGO & SPGO & PPO\\ \hline
    In place Point-In-Time recovery & Yes & No & No & No\\ \hline
    Individual database restore & Yes & No & No & No\\ \hline
    User interface & No & No & Yes & No\\ \hline
    Major version upgrade & Yes & No & Yes & Yes\\ \hline
    Supported Postgres versions & v11 - v15 & v11 - v15 & v12 - v15 & v12 - v14\\ \hline
}

\n{1}{Metrics}
\label{chap:metrics}
This chapter aims to answer the second research question: 'What metrics are suitable for comparing Opereators for lifecycle management in
Kubernetes?'" According to Tom Gilb \cite{gilb1988principles}, the main issue in software attribute requirements is identified not in their functionality, but in their quality. Gilb differentiates these attribute requirements into two categories: Resources (people, time, money), which are always finite, and qualities or benefits (security, performance, usability), which are always fewer than desired.

Knowledge about the functionality that an operator must provide to achieve a certain level of capabilities is obtained from Chapter~\ref{chap:operators}. The most significant functional properties of operators have been detailed in Chapter~\ref{chap:searchForoperators}. With the lifecycle of Postgres and the capabilities of operators now understood, what remains to be examined are their qualitative properties. The upcoming testing will be focused on the proposed qualitative metrics.

\n{2}{Performance}
Performance is a qualitative parameter of a system, defined by the efficiency with which the system utilizes allocated resources. In the case of Postgres, performance can be expressed as the number of transactions executed per unit of time. A higher transaction rate is indicative of superior performance.

By striving for high-performance levels, stakeholders can ensure that Postgres system effectively meet the demands of applications, deliver efficient data processing, and provide a satisfactory user experience.

\n{2}{Reliability}
Reliability is a critical parameter in evaluating the effectiveness of any system. It refers to the degree of reliability and consistency that can be placed on a system to consistently perform its intended functions. A reliable system can be trusted to operate without faults or interruptions and to deliver the expected results under normal operating conditions.

Relying on the system is essential for smooth operations, productivity and customer satisfaction. A system that is not reliable carries risks of outages, data corruption, errors and potential financial or reputational losses.

\n{2}{Usability}
Usability is a key aspect of software design that prioritises user experience and satisfaction. It includes the ability of the system to meet specific user goals in a particular context of use. A usable software system is intuitive and effective because it allows users to achieve their goals with ease and minimal cognitive effort.

By prioritizing usability the stakeholders can promote increased productivity, reduce user errors, and ultimately increase user satisfaction and adoption.

\n{2}{Maintenance}
Activities that are performed after the software is deployed to ensure its correct functionality and performance. Maintenance may include bug fixes, adding new features, performance optimization, updates for compatibility with new systems, etc. Ignoring software maintenance can lead to increased repair costs and reduced system performance over time. Additionally, it can cause system instability, increased vulnerability to security threats, and eventually, potential system failure.

\n{2}{Security}
Software security is a key aspect of modern technology systems. It involves implementing measures to protect software and related data from unauthorized access, manipulation or misuse.
Effective software security requires a proactive approach which addresses potential vulnerabilities and mitigates risks throughout the software lifecycle. This includes secure coding practices, regular security testing and auditing, timely patching and updates, and robust access control mechanisms.

By prioritising software security, stakeholders can protect sensitive information, maintain the integrity of their systems and protect themselves from potential security breaches and malicious activity.


\n{1}{Testing methodology}
This chapter aims to answer the third research question: 'What approach should be taken to determine the degree to which the metrics are
met?'" and presents a high-level overview of the testing methodology. The goal of the methodology is to deliver rules and guidance for test process that produces test reports forming the basis of this evaluation.

\n{2}{Notice}
It is important to notice at the beginning of this chapter that testing as described in \cite{FoundationOfSoftwareTesting} has following seven testing princliples:
\begin{enumerate}
    \item Testing shows the presence of defects, not their absence
    \item Exhausting testing is impossible
    \item Early testing saves time and money
    \item Defects clusters together
    \item Beware of pesticide paradox
    \item Testing is context dependent
    \item Absence of errors is fallacy
\end{enumerate}



Therefore, the test process derived from this methodology as every test process will not exhaustively test the operators and will be depended on thesis context, author bias, and author skills. Because The objective of this thesis is to conduct an extensive evaluation of various Kubernetes
Operators available for Postgres lifecycle management the main scope of this methodology is to deliver test process that will produce test reports that will form the base for this evaluation.

\n{2}{Criteria}
Some metrics are well measurable on their own, while others require further breakdown into multiple sub-metrics in order to obtain the results.
The metrics definied in Chapter \Ref{chap:metrics} have been divided to criteria according to Figure \ref{fig:criteriaDivision}.

\obr{Criteria breakdown}{fig:criteriaDivision}{1}{graphics/test_methodology-2.png}
To keep track of these criteria, each one has been assigned an ID. The list of identified criteria is as follows:

\begin{itemize}
    \item	CP: Performance
          \begin{itemize}
              \item	CP1: Performance analysis
          \end{itemize}
    \item	CR: Reliability
          \begin{itemize}
              \item	CR1: Maturity
                    \begin{itemize}
                        \item	CR1A: Popularity
                        \item	CR1B: Fixed issues
                    \end{itemize}
          \end{itemize}
    \item	CU: Usability
          \begin{itemize}
              \item	CU1: Learnability
                    \begin{itemize}
                        \item	CU1A: Documentation examples
                        \item	CU1B: Required training
                    \end{itemize}

              \item  CU2: Operability
                    \begin{itemize}
                        \item	CU2A: Ease of use
                        \item	CU2B: Monitoring
                    \end{itemize}
          \end{itemize}
    \item	CM: Maintenance
          \begin{itemize}
              \item	CM1: Renewal
          \end{itemize}
    \item	CS: Security
          \begin{itemize}
              \item	CS1: Vulnerability analysis
          \end{itemize}
\end{itemize}

\n{3}{Performance testing}
Performance testing must be conducted using performance analysis. To test the performance, an environment must be set up in which the deployed operator creates a high-availability (HA) Postgres cluster consisting of three nodes.
The test environment must be recreated for each operator being evaluated. Additionally, an equal number of pooler nodes must be created. It is essential that the settings of all Postgres clusters are the same, and only the HA service must be tested.
The performance tests must be carried out at least three times to ensure accurate and reliable results.

\n{3}{Reliability testing}
Reliability must be tested through operator maturity testing.
\n{4}{Maturity testing}
Determining the maturity of operators must be done by assessing their popularity and the ratio of fixed issues. Popularity can serve as an indicator of maturity because widespread recognition and usage of a product often imply reliability, efficiency, and the ability to meet user requirements.
Popularity must be determined by considering the number of stars in the operators' repositories.

Furthermore, the ratio of the number of open issues to the number of resolved issues must be examined. This results shall also be gathered from the operator’s repositories.

By considering both popularity and the ratio of resolved issues, valuable insights can be gained regarding the maturity level of the operators.

\n{3}{Usability testing}
Usability must be tested through operator learnability testing and operability testing.

\n{4}{Learnability testing}
To determine the learnability of operators, a list of Postgres lifecycle events (described in Chapter \Ref{chap:lifecycle}) must be created, and the operator documentation must be examined to ensure that it provides examples for each Postgres lifecycle event. Throughout this process, the number of tools required to successfully use the operator must be recorded to determine the level of training needed.

Learnability testing, which is a crucial aspect of usability testing, must be conducted to assess the ease with which new users can understand and use the operator. The testing aims to achieve two primary objectives. Firstly, it must identify the presence of examples in the documentation, as learning facilitated by examples is generally more effective. Secondly, it must measure the level of learning required to operate the system effectively.

By conducting thorough learnability testing, valuable insights can be obtained regarding the user-friendliness and accessibility of the operators, contributing to the overall evaluation of their usability.

\n{4}{Operability testing}
To test the operability of the operator, the testing methodology involves creating use cases derived from the Postgres lifecycle (Chapter \Ref{chap:lifecycle}), developing corresponding test cases for these use cases, and determining the number of commands necessary to accomplish the desired functionality.
Configuration updates must consist of at least three types of updates: extension installation, change of Postgres configuration parameter, and change of cluster component parameter. These updates are necessary to ensure the flexibility and adaptability of the system.

Additionally, a comprehensive list of required monitoring items must be compiled, and the test case incorporating monitoring activities assesses whether the operator's monitoring adequately includes these items. This comprehensive approach ensures that the operability of the operator is thoroughly assessed, taking into account its functionality, and monitoring capabilities.

\n{3}{Maintenance testing}
Maintenance testing must be performed by measuring the number of commits in the operator's repository. The ratio of these commits to the number of days since the repository was created must be determined. This approach is necessary to calculate the number of commits per unit of time, which is crucial in assessing the rate at which the software is being renewed.

\n{3}{Security testing}
Security testing must be conducted by analyzing the vulnerability of the docker image for each individual operator. As the overall cluster comprises multiple docker containers with various dependencies such as Postgres, pooling, backups, etc., to streamline the testing process, only the operator image will be subjected to vulnerability analysis. This approach allows focusing on the specific security aspects of the operator without testing of the Postgres cluster dependencies.
However, in case of unusual results, other docker containers utilized by the operator may be subject of the vulnerability analysis as well.

\n{2}{Test management process}
According to the IEEE Standard for Software Test Documentation \cite{ieeeTestProcess}, test management processes have three main test processes: test strategy and planning, test monitoring and control, and test completion.
As depicted in Figure \ref{fig:ieeeTestProcess} testing has more than one management test process. The main process is the Organizational process which is further divided into Test management processes that are then devided into Dynamic test processes.

\obr{Test management process relationships \cite{ieeeTestProcess}}{fig:ieeeTestProcess}{1}{graphics/test_management_process.png}

Thesis test process will consist of one management process that will create two managed subprocesses, one for static testing and one for dynamic testing. This test management process will monitor and control subprocesses. Subprocesses will deliver all their deliverables to this main process.

\n{2}{Test strategy and planning}
The output of the test strategy and planing will be the test plan, as the basis for its creation will be the criteria created earlier. Details about activities are described by ISO/IEC/IEE 29119-3.

As depicted on \ref{fig:ieeeTestProcess} test plan is not static but it changes according to monitoring.


\n{2}{Test plan}
\obr{Test plan creation activities \cite{ieeeTestProcess}}{fig:ieeeTestPlan}{1}{graphics/test_plan_process.png}
In order to create a test plan, IEEE proposes the following procedure shown in the figure \ref{fig:ieeeTestPlan} with the idea that some activities can be repeated.

The result of a properly designed test plan should be:
\begin{itemize}
    \item Scope of testing
    \item List of identified risks
    \item Testing strategy
    \item Test environment
    \item Test tools
    \item Test data
    \item Staffing
    \item Scheduling
    \item Required training
    \item Estimates of time and resources
    \item Compliance with all stakeholders
\end{itemize}

\n{2}{Test monitoring and control process}
The role of the test monitoring and control process is to observe the test process and detect deviations from the plan. This process controls the test process throughout its duration. The findings are then used to modify the test plan.

To avoid unnecessary bureaucracy, where the manager and tester are the same person, and consequently the testing progress would be reported to the person who is also filing it, there will be no status reports during the test process.

\n{2}{Test completion process}
The test completion process as depicted in the figure \ref{fig:ieeeTestCompletition} will be used in testing after each test the test competition report will be created and delivered to a higher level.
\obr{Test completion process \cite{ieeeTestProcess}}{fig:ieeeTestCompletition}{1}{graphics/test_completation_report.png}


\n{2}{Dynamic and static test processes}
According to ISTQB \cite{FoundationOfSoftwareTesting} there are two types of tests. Static and dynamic. The main difference is that the static technique does not execute the tested software, but the dynamic does. The testing process will utilize both techniques using the dynamic testing process depicted in figure \ref{fig:ieeeTestDynamicTestProcess}.

\obr{Dynamic test processes \cite{ieeeTestProcess}}{fig:ieeeTestDynamicTestProcess}{1}{graphics/test_dynamic_test_processes.png}

\n{2}{Test design and implementation processes}
Test design and implementation process must follow the process depicted in the figure \ref{fig:ieeeTestDesignAndImplementation}. Test design techniques should be used to derive test cases. Test cases must be traceable to criterion. This process can be reentered multiple times and must meet the completion criteria specified in the test plan.
\obr{Test design and implementation \cite{ieeeTestProcess}}{fig:ieeeTestDesignAndImplementation}{1}{graphics/test_design_and_implementation.png}


\n{2}{Test environment and data management processes}
Based on the test plan all the environments must be established and well mainteained.

\n{2}{Test execution process}
Test execution process depicted in \ref{fig:ieeeTestExecutionProcess} must be followed. After the test execution, the execution log should be delivered. But since all roles are performed by the same person, the test execution log will not be created or delivered.
Details about activities are described by ISO/IEC/IEE 29119-3.

\obr{Test execution process \cite{ieeeTestProcess}}{fig:ieeeTestExecutionProcess}{1}{graphics/test_execution_process.png}

\n{2}{Test incident report process}
The process for reporting test incidents, as depicted in Figure \ref{fig:ieeeTestExecutionProcess}, must be followed. It is important to note that the purpose of testing is not to finding incidents, but rather to test the capabilities of the system. As such, the term "finding" will be used in cases where it is more appropriate.

\n{2}{Level of detail}
Please note that this chapter provides only a high-level overview of the testing methodology. More detailed information can be found in ISO/IEC/IEEE 29119-2\footnote[5]{https://standards.ieee.org/ieee/29119-2/7498/}. If there are any doubts regarding the testing process, this standard should be used as a guideline, but it is not necessary to strictly adhere to it in its entirety.

\cast{APPLICATION OF THEORY}
\n{1}{Test process}
\label{chap:testProces}
As stated in the methodology, the main test process with a general test plan was created. The general test plan can be seen in Appendix AI. This process was divided into two subprocesses: one for static testing and one for dynamic testing.

According to the priorities stated in the general test plan, the first test process to be conducted was the static test process.

\obr{Test process}{fig:testingTestProcess}{1}{graphics/test_process-2.png}

\n{2}{Tools}
During the creation of the test plan, the following tools were identified as essential for conducting effective testing.
\begin{itemize}
    \item \textbf{Bash}: Bash, short for "Bourne Again SHell," is a command-line interpreter and scripting language used in Unix-like operating systems.
    \item \textbf{Kubectl}: Command-line utility for managing Kubernetes clusters.
    \item \textbf{Terraform}: Infrastructure as code tool for provisioning and managing cloud resources.
    \item \textbf{Trivy}: Vulnerability scanner specifically designed for containerized environments.
    \item \textbf{PgBench}: Benchmarking tool for Postgres systems that allows for the simulation of various database workloads to measure and evaluate performance.
\end{itemize}

Additional tools were identified and incorporated into the testing process at later stages.
\begin{itemize}
    \item \textbf{Git}: Distributed version control system.
    \item \textbf{Kustomize}: Configuration management tool for Kubernetes.
    \item \textbf{Snyk}: Security scanning tool that helps identify and fix vulnerabilities in open-source libraries and container images.
\end{itemize}

\n{2}{Static test process}
The static test process was divided into three separate subprocesses, each with its own test plan.
\n{3}{Reliability and maintenance}
\label{chap:testReliabilityAndMaintenance}
Reliability and maintenance testing consists of two parts: renewal and maturity. Both of these parts form the first static testing process because they share the same test items and therefore dividing them into separate processes doesn't make sense. This process was designed to provide the necessary information for decision-making regarding the maintenance quality and maturity level of the operators. A detailed test plan and further specifics of this process can be found in Appendix A II.

During the test process, it was observed that PPO does not use the repository to track issues, instead it uses Jira. Another challenge arose when trying to determine the repository creation date and getting the number of commits in repository. Retrieving this data from Gitlab or Github proved to be quite difficult. As a workaround, all the repositories were cloned, and the date of the initial commit was obtained using the command mentioned in Listing \ref{lst:gitLogReverse}.
Likewise, the total number of commits was determined using the command mentioned in Listing \ref{lst:commitsCount}. Due to these modifications, the test plan was subsequently revised.

\begin{lstlisting}[language=bash, caption={Reverse git log}, label={lst:gitLogReverse}]
        $ git log --reverse
\end{lstlisting}

\begin{lstlisting}[language=bash, caption={Commits count}, label={lst:commitsCount}]
    $ git rev-list --all -count
\end{lstlisting}

The results of this repository analysis are presented in Table \ref{tab:operatorTestRelAndMaint}. As can be seen, PGO and PPO share the same creation date. This is due to the fact that PPO is a fork of PGO, as metioned earlier in Chapter~\ref{chap:ppo}.

\tab{Operator repository analysis}{tab:operatorTestRelAndMaint}{.5}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Repo creation	&  27th Feb 2017 & 18th Feb 2020	&  29th May 2019	&  27th Feb 2017 \\ \hline
    Test date	&   1st May 2023 & 1st May 2023	&  1st May 2023	&  1st May 2023 \\ \hline
    Stars	& 3258	& 1198	 & 84 &	149 \\ \hline
    Issues	&  1884	& 764	& 1959	& 317 \\ \hline
    Issues fixed	&   1755	& 691 &	1514	& 282 \\ \hline
    Commits	&   5582	& 3362 &	7208	& 4689 \\ \hline
}
\n{3}{Usability: Learnability}
\label{chap:learnability}
The learnability testing process was divided into two distinct aspects: the first one being the training required to operate with the operators, and the second one being the presence of examples concerning Postgres lifecycle events in the operator's documentation. Following the guidelines of the test plan (available in Appendix A III), a checklist was created. The documentation for each operator was thoroughly reviewed, resulting in the findings presented in Tables \ref{tab:operatorTestTraining} and \ref{tab:operatorTestDocExamples}.

\tab{Training needed}{tab:operatorTestTraining}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    1st training & Kubectl & Kubectl & Kubectl & Kubectl \\ \hline
    2nd training & Kustomize & Helm	& Helm & Helm\\ \hline
    3rd training & -	& Cnpg	& - & - \\ \hline
}

\tab{Documentation examples}{tab:operatorTestDocExamples}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Cluster creation &	Yes &	Yes &	Yes &	Yes \\ \hline
    Minor upgrade &	Yes &	Yes &	No &	Yes \\ \hline
    Major upgrade &	Yes &	No &	No &	Yes \\ \hline
    Backup &	Yes &	Yes &	Yes &	Yes \\ \hline
    Restore &	Yes &	Yes &	Yes &	Yes \\ \hline
    Monitoring &	Yes &	Yes &	Yes &	Yes \\ \hline
    Vertical scaling &	Yes &	Yes &	Yes &	No \\ \hline
    Horizontal scaling &	Yes &	Yes &	Yes &	Yes \\ \hline
    Configuration Update &	Yes &	Yes &	Yes &	Yes \\ \hline
    Uninstall&	Yes &	Yes &	Yes &	No \\ \hline
}

\n{3}{Security}
To proceed with the security testing, a vulnerability analysis test plan was created (which can be found in Appendix A IV), and the process was carried out according to this plan.

The test items for this process were identified, the test tool was installed, and a test procedure was developed which consisted of four test cases - one for each operator (details in Appendix A IV).
Each operator was then tested according to this procedure, with the overall vulnerability results presented in Table \ref{tab:operatorTestVulnTrivy}.

During this test process the security scanner Trivy was unable to detect any vulnerabilities in CNPGO's container image utilizing Debian 11.6.
The rest of the operators are using the Red Hat 8.7 container image, which resulted in almost identical vulnerability scores.
Unfortunately, the implementation of Red Hat 8.7 in both PGO and SPGO has a high vulnerability in openssl-libs (CVE-2023-0286).
More details about this vulnerability can be found here: \url{https://avd.aquasec.com/nvd/2023/cve-2023-0286/}.

\tab{Trivy vulnerability analysis results}{tab:operatorTestVulnTrivy}{1}{|l|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO & Debian 11.6 \\ \hline
    Critical &	0	& 0 &	0 &	0 &	1 \\ \hline
    High &	1 &	0 &	1 &	0&	17 \\ \hline
    Medium &	40 &	0 &	41 &	35&	6  \\ \hline
    Low &	36 &	0 &	36 &	36&	59  \\ \hline
    Unkown &	0 &	0 &	0 &	0 &	0 \\ \hline
}

The absence of any detected vulnerabilities in CNPGO by Trivy raised a question: Is Trivy accurately scanning this image? To address this concern, the test plan was updated to include an additional test case.
This test involved scanning the base image of CNPGO to ascertain if Trivy could detect any vulnerabilities in Debian 11.6.

Debian image was scanned with the results presented in Table \ref{tab:operatorTestVulnTrivy} which can be interpreted in two ways. The first interpretation suggests that CNPGO might be using Debian but has effectively removed or mitigated the vulnerable parts. The second interpretation considers the possibility that Trivy may not be able to accurately identify Debian vulnerabilities within CNPGO.

To eliminate the second interpretation an additional vulnerability analysis tool, Snyk, was incorporated into the testing process. The test plan was subsequently adjusted, and the images were scanned using Snyk. This alternative method produced results similar to those from the Trivy scanning for each operator. However, there were exceptions in terms of High severity issues;
Snyk identified three additional vulnerabilities in the openssl-libs of PGO and SPGO and provided different results for Debian.

\tab{Snyk vulnerability analysis results}{tab:operatorTestVulnSnyk}{1}{|l|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO & Debian 11.6 \\ \hline
    Critical & 0 & 0 &	0 &	0 &	0 \\ \hline
    High & 4 &	0 &	4 &	0 &	1 \\ \hline
    Medium &38 &	0 &	39 &	36 &	2  \\ \hline
    Low &	39 &	0 &	39 &	39 &	48 \\ \hline
    Unkown &	0 &	0 &	0 &	0 &	0 \\ \hline
}

Even after conducting these two testing rounds, it was surprising to find that the CNPGO operator did not have any vulnerabilities. Presumably, this indicates that the operator is currently free from vulnerabilities as of the testing date. However, to ensure that this false result does not create the impression that no vulnerabilities would be introduced into the Kubernetes cluster when deploying a Postgres cluster using this operator, an additional round of testing was carried out. This series of tests involved deploying the operator onto a Kubernetes cluster, creating a Postgres cluster with primary and standby replicas, a connection pooler, and a backup.
To ensure consistency in the Postgres versions, Postgres version 14.7 was utilized whenever possible. In cases where version 14.7 was not feasible, version 14 was employed, and the choice of version was left to the discretion of the operator.

The entire cluster was then scanned using Trivy, and the number of vulnerabilities in the namespace where the Postgres cluster was deployed was recorded in Table \ref{tab:operatorTestVulnK8s}. In cases where it was evident that the vulnerabilities were the same in both replicas, only one instance of the vulnerability was counted.

\tab{Results of the Postgres cluster deployment using the operator}{tab:operatorTestVulnK8s}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO\\ \hline
    Critical & 0 &	5 &	0 &	0 \\ \hline
    High & 54 &	55 &	16 &	10 \\ \hline
    Medium & 1171 &	26 &	461 &	467  \\ \hline
    Low &	2334 &	115 &	433 &	744 \\ \hline
    Unkown &	0 &	0 &	0 &	0  \\ \hline
}
During this testing, a significant number of additional vulnerabilities were identified, particularly critical vulnerabilities in the CNPGO's image ghcr.io/cloudnative-pg/postgresql:14.7. One recurring vulnerability was found in Python 3.9 (CVE-2021-29921), and more information about it can be found at \url{https://avd.aquasec.com/nvd/cve-2021-29921}. Additionally, a vulnerability was discovered in SQlite (CVE-2019-8457), and further details can be found at \url{https://avd.aquasec.com/nvd/cve-2019-8457}.

Due to the presence of multiple docker containers (up to 6) within clusters created by operators, it is crucial to assess the severity of errors in the cluster rather than simply counting the number of errors. This is necessary as some errors may occur repeatedly, leading to inflated error counts. Furthermore, it is important to note that the same vulnerabilities can be present multiple times within a single image. For instance, the critical Python 3.9 bug (CVE-2021-29921) was identified four times in the CNPGO Postgres 14.7 image.

Complete results of Trivy and Snyk scans can be located in the thesis repository\footnote[6]{https://github.com/Ovec/Bachelors-thesis} folder at \url{tests/vulnerability_analysis}. The overall results are presented in Tables \ref{tab:operatorTestVulnTrivy}, \ref{tab:operatorTestVulnSnyk} and \ref{tab:operatorTestVulnK8s}

\n{2}{Dynamic test process}

The dynamic test process was divided into two separate subprocesses, each with its own
test plan.

\n{3}{Environments}
Two environments were used for the dynamic test process, each designed for specific test types based on resource requirements:

\begin{enumerate}
    \item \textbf{Kind Kubernetes Cluster}: Kind is a tool that creates Kubernetes clusters within Docker, Kind was utilized for less resource-intensive tests. This setup was run on a first-generation M1 MacBook Air equipped with 8 GB of RAM and a 500 GB disk.
    \item \textbf{Google Kubernetes Engine (GKE)}: More resource-demanding tests, such as performance testing, were conducted on a robust Google Kubernetes Engine cluster, consisting of three e2-standard-2 nodes. Each node with 2 virtual CPUs and 8GB of RAM. Additionally, a standalone Postgres node with PgBench was deployed to this cluster for performance analysis.
\end{enumerate}

The configurations for Kind and the Terraform plans used for deploying the GKE cluster, as well as the commands used for both deployments, can be found in the repository directory located at \url{tests/environments}.

\n{3}{Usability: Operability}
\label{chap:operability}

The operability testing process was divided into two distinct aspects: the first one
being ease of use, and the second one being the quality of provided monitoring.
Following the guidelines of the test plan, use cases derived from Postgres lifecycle and testing procedure was created (all available in Appendix A V).

\n{4}{Ease of use}
\label{chap:easeOfUse}
Testing the ease of use involved testing each operator for the functionality outlined by the use cases and quantifying the number of commands required to implement these functions.
This form of testing tested the functional aspects of the operator in conjunction with its ease of use.
To achieve this, three types of shell scripts were utilized: before.sh, test.sh, and cleanup.sh.

The before.sh script prepared the environment for the test, the test.sh script executed the test itself, and the cleanup.sh script restored the environment to its state prior to the test.
The precondition for testing the operators was a functioning Kubernetes cluster with kubectl configured for this cluster.
Additionally, to perform tests on the Postgres clusters, it was necessary for the operator to be installed in the cluster.

In cases where it was necessary to verify functionality, this verification was made manually. Result are presented in Table \Ref{tab:operatorTestEaseOfUse}.

\n{5}{SPGO}
The reason why SPGO has a zero value in most test cases is due to its user-friendly interface.
Most tasks can be accomplished directly within this interface, eliminating the need for using the terminal.
Even complex tasks, such as performing a major version upgrade, can be easily carried out via this interface.
An example of this user interface is shown in Figure \ref{fig:spgoUI7}.
Additional images can be found in the repository folder at \url{doc/graphics/monitoring/SPGO}.

\obr{SPGO's user interface}{fig:spgoUI7}{1}{graphics/monitoring/SPGO/spgo_ui7.png}

\n{5}{Cluster major version upgrade}
The upgrade to a new major version appears to be the most challenging task for each operator. While SPGO handled the cluster upgrade seamlessly, PGO required four steps to proceed with the major version upgrade. On the other hand, CNPGO claimed to be capable of an "Offline import of existing PostgreSQL databases, including major upgrades of PostgreSQL". This process involves dumping the database and restoring it to a new cluster, which can be done with any cluster and is not considered a major upgrade. PPO declared in their documentation that they are capable of automatic updates even between versions. However, this Jira issue (\url{https://jira.percona.com/projects/K8SPG/issues/K8SPG-254?filter=allopenissues}) suggests otherwise.

\n{5}{PPO}
During the cluster update test case, specifically the update of max\_wal\_size, PPO experienced a failure.
Despite updating the cluster configuration, the max\_wal\_size of Postgres remained unchanged.
PPO also was the only operator that did not support the PostGis extension for Postgres.

\n{5}{CNPGO}
During the testing procedure, it was noted that CNPGO didn't require the CNPG plugin, as initially mentioned in Chapter \ref{chap:learnability}, to perform any operation. Consequently, the necessity for training related to this plugin was reevaluated, leading to its removal. The updated results are presented in Table \ref{tab:operatorTestUpdatedTraining}.



\tab{Ease of use}{tab:operatorTestEaseOfUse}{.5}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Operator installation & 2 & 1 & 2 & 3 \\ \hline
    Cluster installation & 1 &	2 &	0 &	1\\ \hline
    Cluster monitoring & 2 & 4 & 3 & 5 \\ \hline
    Cluster vertical scaling & 1 &	1 &	0 &	1 \\ \hline
    Cluster horizontal scaling & 1 & 1 & 0 & 1\\ \hline
    Cluster connection pooling & 1 & 1 & 0 & 0 \\ \hline
    Cluster extension install & 1	& 1 & 0 & - \\ \hline
    Cluster number of connections increase & 1 &	1 &	0 &	2 \\ \hline
    Cluster max\_wall\_size increase & 1 & 1 & 0 & - \\ \hline
    Cluster scheduled backup & 1 & 1 & 0 & 1 \\ \hline
    Cluster ad-hoc backup & 2	& 1 & 0 & 1 \\ \hline
    Cluster restore & 2 & 1 & 1 & 1 \\ \hline
    Cluster minor version upgrade & 1 &	1 &	0 &	1 \\ \hline
    Cluster major version upgrade & 4 &	- & 0 & - \\ \hline
    Operator uninstall & 1	& 1 & 1 & 1 \\ \hline
    Cluster uninstall  & 1 & 1 & 0	& 1 \\ \hline
}

\tab{Training needed: udpated}{tab:operatorTestUpdatedTraining}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    1st training & Kubectl & Kubectl & Kubectl & Kubectl \\ \hline
    2nd training & Kustomize & Helm	& Helm & Helm\\ \hline
}


\n{4}{Monitoring}
\label{chap:monitoring}
During the monitoring deployment test case, screenshots of each monitoring system were taken. The results are presented in Table \ref{tab:operatorTestMonitoring}.
These screenshots can be found in the repository folder at \url{doc/graphics/monitoring}.

All of the operators, except for PPO, use the traditional Grafana Prometheus monitoring stack, while PPO uses the Percona Monitoring and Management solution.
This Percona monitoring system is quite extensive, but its coverage of parameters is comparable to that of the less extensive CNPGO monitoring system.

\tab{Monitoring}{tab:operatorTestMonitoring}{.5}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Health & Yes &	Yes &	Yes &	Yes\\ \hline
    Query performance & Yes	& Yes	& No	& Yes\\ \hline
    Number of connections & Yes	& Yes	& Yes	 &Yes \\ \hline
    Locks & Yes	& Yes	& Yes	& Yes \\ \hline
    Index hit  & No	& No	& No	& No\\ \hline
    Cache hit & Yes	& No	& Yes	& Yes \\ \hline
    Disk space usage & Yes	& Yes	& No	& Yes \\ \hline
    CPU and memory usage & Yes	& Yes	& Yes	& Yes \\ \hline
    WAL generation rate & Yes	& Yes & No	& Yes \\ \hline
    Replication lag & Yes	& Yes	& No	& No \\ \hline
    Errors and logs & No	& No	& No	& Yes \\ \hline
    Backup and recovery & Yes	& Yes	& Yes	& No \\ \hline
}

\n{3}{Performance}
\label{chap:performance}
To measure the performance of the Postgres cluster, an operator was deployed to the GKE cluster. Subsequently, a high availability Postgres cluster was established, consisting of three nodes (one primary and two replicas), along with three connection pooler instances.
A simplified representation of this cluster configuration is depicted in Figure \ref{fig:testingGKEperformance}.

\obr{Kubernetes performance configuration}{fig:testingGKEperformance}{1}{graphics/performance_setup2.png}

For each operator test, the Kubernetes cluster was recreated to ensure a clean starting point. A Postgres cluster with three nodes (one primary and two replicas) was deployed, along with three connection pooler instances.

To standardize the settings across each cluster, configurations were calibrated in line with recommendations provided by the pgTune\footnote[7]{https://pgtune.leopard.in.ua}. The adjusted settings were as follows:
\begin{itemize}
    \item max\_connections: 200
    \item shared\_buffers: 1536MB
    \item effective\_cache\_size: 4608MB
    \item maintenance\_work\_mem: 384MB
    \item checkpoint\_completion\_target: 0.9
    \item wal\_buffers: 16MB
    \item default\_statistics\_target: 100
    \item random\_page\_cost: 4
    \item effective\_io\_concurrency: 2
    \item work\_mem: 3932kB
    \item min\_wal\_size: 1GB
    \item max\_wal\_size: 4GB
\end{itemize}

After the creation of the Postgres cluster, each cluster was tested using the PgBench tool. This tool was configured to execute 10,000 transactions across 25 concurrent clients and utilizing 10 threads. This benchmark was directed towards the Postgres cluster's pooler service. This procedure was replicated twice more for thoroughness.

The results presented in Table \ref{tab:operatorPerformance} only display the transactions per second, as these offer a sufficient indication of the operator's performance. For a comprehensive view of the results from this benchmark, along with the test process details, please refer to Appendix A VI.

The commands executed throughout this procedure, along with the complete configuration of the operators, can be located in the \url{tests/performance} directory in the repository.

\n{3}{Issues with CNPGO}
CNPGO was the only one unable to execute 250,000 transactions in each run due to an error (client 6 script 0 aborted in command 4 query 0: FATAL: query wait timeout, SSL connection has been closed unexpectedly).
This error usually occurs when a query takes too long to execute, leading to a timeout. The SSL connection is then closed unexpectedly, causing the transaction to fail. This might suggest that CNPGO is struggling with performance or network stability in this particular scenario.

\n{3}{Issues with SPGO}
The possible reason for SPGO's low transactions per second score is that a cluster profile is needed to deploy an SPGO cluster. When this profile was correctly set, Google Kubernetes Engine was unable to deploy the cluster. By gradually reducing these values, the available resources were eventually found, but these settings were probably too low for optimal performance (500m CPU and 2Gi RAM). Despite the cluster showing that it had more memory and CPU allocable, as may be seen in Figure \ref{fig:testingGKEnodes}, the reduced resource allocation might have constrained SPGO's performance.

\n{3}{Issues with PPO}
As mentioned in Chapter~\ref{chap:easeOfUse}, changes to PPO's configuration do not affect the cluster, therefore, the cluster was not modified during this test. This means that the performance results for PPO are based on its default configuration settings.


\tab{Performance analysis}{tab:operatorTestPerformance}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    First run &	544.91 tps	& 403.70 tps	& 284.39 tps &	401.46 tps \\ \hline
    Second run &	543.29 tps &	402.54 tps &	279.89 tps &	392.04 tps\\ \hline
    Third run &	538.51 tps &	392.63 tps &	309.16 tps &	387.79 tps \\ \hline
    Mean &	542.24 tps &	399.62 tps &	291.15 tps &	393.77 tps \\ \hline
}

\obr{GKE nodes details}{fig:testingGKEnodes}{1}{graphics/gke_nodes.png}


\n{1}{Evaluation}
This chapter aims to answer the fourth research question: ’How do the operators perform when evaluated according to the chosen metrics?’".
It will utilize the findings presented in Chapter \ref{chap:testProces} to conduct a comprehensive evaluation of each quality attribute associated with the operators.
Each criterion will be quantified and discussed in detail.
The cumulative results will be presented at the end of this chapter.

\n{2}{Measuring rule}
In this chapter, the operators will be evaluated. When a dedicated measuring tool is not available, the operators will be compared against each other to provide a relative measure of their attributes.
This comparative analysis will help illuminate their relative strengths and weaknesses.

\n{2}{Reliability}
\n{3}{Maturity}

In order to determine the maturity of the system, data was statically collected from the repositories of each operator (Chapter \ref{chap:testReliabilityAndMaintenance}).
This included the popularity of the operators, the number of issues they had, and the number of issues that were resolved.
The maturity was then determined based on the popularity of the operators and the ratio of resolved issues.

\n{4}{Popularity}
After examining the popularity by the number of stars, it is clear that PGO far exceeds the others (results presented in Table \ref{tab:operatorPopularity}). In contrast, CNPGO shows much lower popularity.
The remaining operators show marginal levels of popularity to the level of popularity that PGO or CNPGO have received.
If the ranking were based solely on a popularity contest, PGO would clearly win. However, the evaluation requires a more comprehensive examination beyond simply measuring popularity.
\tab{Popularity of operators}{tab:operatorPopularity}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Popularity (stars)	& 3258	& 1198	 & 84 &	149 \\ \hline  \hline
    Popularity ratio	& 100\% &	37\%	& 3\% &	5\% \\ \hline
}

\n{4}{Fixed issues}
As presented in Table \ref{tab:operatorIssues}, PGO, CNPGO, and PPO are shown to have resolved a significant number of issues, with PGO registering the highest ratio of fixed issues. On the other hand, despite being less popular and more recent than PGO, SPGO has a larger number of reported issues, many of which remain unresolved (445 in total). This might suggest that the development of SPGO is still in progress.
\tab{Operators issues}{tab:operatorIssues}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Total issues	&  1884	& 764	& 1959	& 317 \\ \hline
    Issues fixed	&   1755	& 691 &	1514	& 282 \\ \hline  \hline
    Fixed issues ratio	&  93\% &	90\% &	77\% &	89\% \\ \hline
}

\n{3}{Reliability}
The overall reliability of the operators was determined maturity. These results are presented in Table \ref{tab:operatorReliability}.

Due to its high popularity and impressive ratio of fixed issues, PGO's results are outstanding, suggesting that it can be considered the most reliable operator among all.
The results for CNPGO are good, while those for SPGO and PPO can be regarded as fair.

\tab{Operators reliability}{tab:operatorReliability}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Reliability	& 97\%	& 64\% &	40\%	& 47\% \\ \hline
}

\n{3}{Maintenance}
\n{4}{Renewal}
The overall renewal rate of the operators, as presented in Table \ref{tab:operatorRenewal}, was calculated based on the number of commits since the repository's creation date. SPGO achieved the highest average ratio of commits per day, at 5.03.

Interestingly, this contrasts with the operator Fixed Issues, where SPGO scored the lowest, suggesting that it is still in the development stage.
A potential explanation for this could be that SPGO utilizes the 'issues' function for internal project management purposes,
rather than exclusively for issue tracking, or for the frequent updating of SPGO’s user interface.

PGO and PPO share the same lifetime since PPO was forked from PGO. However, PGO has a higher rate of commits per day, indicating that it is updating more rapidly than PPO.

The overall results for this attribute are high, with values greater than 2 indicating that all of the operators are frequently updated.
When comparatively analyzed, SPGO is updated more than twice as frequently as PGO and PPO, and almost 1.8 times more frequently than CNPGO.

\tab{Operators renewal}{tab:operatorRenewal}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Lifetime (days) &   2254 &  1168	&  1433	&  2254 \\ \hline
    Sum of commits	&   5582	& 3362 &	7208	& 4689 \\ \hline
    Commits/day	&   2.48	& 2.88	& 5.03	& 2.08 \\ \hline  \hline
    Renewal / maintenance &   49\%	&  57\%	&  100\%	& 41\% \\ \hline
}

\n{2}{Usability}
Usability testing was conducted in two separate test processes: the first being the static test process described in Chapter \ref{chap:learnability}, which aimed to review the documentation,
and the second being the dynamic test process outlined in Chapter \ref{chap:operability}, which aimed to test the operator's operability.
The results from both processes will be evaluated in this chapter.

\n{3}{Learnability}
Learnability was divided into two parts: one evaluating the required training for each operator, and the other assessing the presence of examples in the documentation.
Both aspects are tested in Chapter \ref{chap:learnability}.

\n{4}{Required training}
The training required presented in Table \ref{tab:operatorTraining} was calculated as the additional training needed to work with each operator, over and above knowledge of Kubectl (the Kubernetes command-line tool).
Each additional tool required to work successfully with the operator resulted in a 5\% score decrease. The choice of a 5\% score decrease for each additional tool required is a subjective decision made based on considerations of practicality, fairness, and relative impact. While

Overall, the required training to work with operators is minimal. For PGO, the only additional software needed, apart from Kubectl—which is essential for working with Kubernetes clusters—is Kustomize.
For the rest of the operators, Helm is required to install the monitoring stack. It can therefore be asserted that proficiency with Kubernetes equates to the ability to work with operators.

\tab{Training needed}{tab:operatorTraining}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Required Training &  95\%	& 95\% &	95\% &	95\%\\ \hline
}

\n{4}{Documentation examples}
As indicated in Table \ref{tab:operatorTestDocExamples}, examples in the documentation are prevalent across all operators. SPGO, however, is at a slight disadvantage.
Despite having a Graphical User Interface capable of managing the entire cluster and efficiently handling all cluster operations, it does not provide examples for major and minor upgrades.
These could be easily configured via the GUI, making it beneficial if the provided examples indicate that the respective functionality can be conveniently implemented using the GUI.
Given that there are ten examples, each one has been assigned a value contributing 10\% towards the total score.

Examples in the documentation were widely presented in the operators' documentation.
PGO achieved the highest rate, presenting all the examples in its documentation and thus offering the most helpful guidance.
Although CNPGO has extensive documentation, it lacks some examples, rendering it slightly less helpful than PGO's.
Both SPGO and PPO were missing even more examples, but despite this, the level of detail in the available examples was still high.

\tab{Documentation examples}{tab:operatorDocExamples}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Documentation examples &	100\%	& 90\%	& 80\%	& 80\% \\ \hline
}


\n{4}{Overall learnability}
The overall learnability rating of the operators, as presented in Table \ref{tab:operatorLearnability}, was calculated as the average of the scores from the required training and the documentation examples.

Overall, the learnability levels are quite high, suggesting that all of the operators are relatively easy to learn.

\tab{Learnability of operators}{tab:operatorLearnability}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Learnability	& 98\% &	93\% &	88\% &	88\%\\ \hline
}


\n{3}{Operability}
\label{chap:testOperatbility}
Operability was divided into two parts: one evaluating the ease of use for each operator, and the other assessing the quality of monitoring. Both aspects are tested in Chapter \ref{chap:operability}.

\n{4}{Ease of use}
The number of commands executed to achieve the objective was counted for each test case. In instances where an operator did not provide the necessary functionality, or the functionality was malfunctioning, the number of steps required was designated as 10.
This allocation is due to the significant effort that would be required to realize this functionality, or the possibility that it might not be achievable at all.

Due to its GUI, SPGO is the easiest operator to use. In comparison to SPGO's ease of use, the ease of use level of the remaining operators is relatively poor.

\tab{Ease of use}{tab:operatorEaseOfUse}{.5}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Sum of commands & 23 & 29 & 7 & 49 \\ \hline
    Ease of use &	30\% & 24\% &	100\% &	14\% \\ \hline
}



\n{4}{Monitoring}
The quality of monitoring was tested in Chapter \ref{chap:monitoring}. Each operator was evaluated based on the number of necessary attributes covered in the monitoring.

PGO has the highest monitoring capabilities, followed closely by CNPGO and PPO, both of which also demonstrate good monitoring abilities. Although SPGO is equipped with a GUI, its monitoring performance is only fair.


\tab{Monitoring}{tab:operatorMonitoring}{.5}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Monitoring &	83\% & 75\% &	50\% &	75\% \\ \hline
}

\n{4}{Overall operability}
The overall operability rating of the operators, as presented in Table \ref{tab:operatorOperability}, was calculated as the average of the scores from the ease of use and the monitoring.

Owing to its high score in ease of use, SPGO has the highest operability among the operators. PGO and CNPGO exhibit similar levels of operability, while PPO, with the lowest score, can be considered the least operable.

\tab{Operability}{tab:operatorOperability}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Ease of use &	30\% & 24\% &	100\% &	14\% \\ \hline
    Monitoring &	83\% & 75\% &	50\% &	75\% \\ \hline  \hline
    Operability &	57\% &	50\%	& 75\%	& 45\% \\ \hline
}

\n{2}{Overall usability}
The overall usability was calculated as the average of learnability, as presented in Table \ref{tab:operatorLearnability}, and operability, as presented in Table \ref{tab:operatorOperability}.

The results, as shown in Table \ref{tab:operatorUsability}, indicate that SPGO is the most usable operator among all, followed by PGO and then CNPGO, with PPO being the least usable.
Nevertheless, the overall usability of the operators is at a good level.


\tab{Usability}{tab:operatorUsability}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Learnability & 98\% &	93\% &	88\% &	88\% \\ \hline
    Operability &	57\% &	50\%	& 75\%	& 45\% \\ \hline \hline
    Usability &	78\%	& 73\%	& 82\%	& 67\% \\ \hline
}

\n{2}{Security}
The results of the vulnerability analysis, as presented in Table \ref{tab:operatorTestVulnTrivy} and \ref{tab:operatorTestVulnK8s}, were assigned quantitative scores based on the severity of the vulnerabilities. The absence of vulnerabilities was given a score of 100\%, unknown vulnerabilities received a score of 80\%, vulnerabilities with low severity were rated at 60\%, medium severity at 40\%, high severity at 20\%, and critical vulnerabilities were assigned a score of 0\%. It is worth noting that the results from Table \ref{tab:operatorTestVulnSnyk} were not included separately, as they were the same as the results from Table \ref{tab:operatorTestVulnTrivy}.
The results from both tables were quantified, and the average score was calculated to provide an overall result, which is displayed in Table \ref{tab:operatorVuln}.

Although CNPGO was the only operator that did not have any vulnerabilities, its dependencies for cluster creation do contain critical vulnerabilities. PGO and SPGO operators have high severity vulnerabilities both in the operators themselves and in the Postgres cluster they create. The PPO operator has the highest vulnerability of medium severity, but the cluster created by it contains high severity vulnerabilities.

Evaluating the extent to which these vulnerabilities pose a threat to the overall security of the Kubernetes cluster, both externally and internally, is beyond the scope of this thesis and depends on whether the Postgres cluster will be accessible from the internet or only within the Kubernetes environment and other parts of the Kubernetes cluster.
From the perspective of this thesis, it can be concluded that the CNPGO operator itself is not vulnerable, but it creates a highly vulnerable Postgres cluster. Other operators are vulnerable and create vulnerable clusters.

\tab{Vulnerability analysis}{tab:operatorVuln}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Operator vulnerabilities &	20\%	& 100\%	 & 20\%	& 40\% \\ \hline
    Cluster vulnerabilities &	20\%	& 0\%	& 20\%	 & 20\% \\ \hline \hline
    Security &	20\% &	50\%	& 20\%	& 30\% \\ \hline
}

\n{2}{Performance}
According to the results presented in Table \ref{tab:operatorTestPerformance} the most performant operator PGO received a score of 100\% in this test, while the other operators were assigned scores proportionally based on their performance relative to PGO.

\tab{Operators performance}{tab:operatorPerformance}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Performance &	100\%	& 74\%	& 54\%	& 73\% \\ \hline
}


\n{2}{Overall quality of the operators}
The overall quality, according to the metric defined in Chapter \ref{chap:metrics}, was calculated as the average of these metrics and is presented in Table \ref{tab:operatorOveralQuality}.
According to these results, the PGO operator can be considered the highest quality among the evaluated operators, followed by CNPGO, SPGO, and finally PPO.


\tab{Overall quality of the operators}{tab:operatorOveralQuality}{1}{|l|l|l|l|l|}{
    \hline
    & PGO	& CNPGO	& SPGO & PPO \\ \hline
    Performance &	100\% & 74\% & 54\% & 73\% \\ \hline
    Reliability &	97\% & 64\% & 40\% & 47\%  \\ \hline
    Usability &	78\% &	73\% &	82\% &	67\%  \\ \hline
    Maintenance &	49\% & 57\% & 100\% & 41\%   \\ \hline
    Security &	20\% & 50\% & 20\% & 30\%  \\ \hline \hline
    Overall quality &	68.8\%	& 63.6\% & 59.2\% &	51.6\%   \\ \hline
}


% ============================================================================ %
\nn{Conclusion}

In this final chapter of the thesis, the objective outlined in Chapter \ref{chap:objectives} is aimed to be met by delivering clear and comprehensive recommendations for stakeholders on which operators are best suited based on defined metrics.

The metrics for comparing operators were established in the previous chapters, and the operators were tested against these metrics, with the measured values being evaluated.

Each operator possesses its strengths and weaknesses across different metrics. The optimal choice depends on which factors are of the greatest importance to the stakeholders.

\begin{itemize}
    \item If performance is a priority, Crunchy Postgres for Kubernetes (PGO) excels, achieving the highest scores (100\%) and can therefore be considered the best choice in terms of performance.
    \item When reliability is a key consideration, Crunchy Postgres for Kubernetes again achieves top marks (97\%), marking it as the most reliable operator.
    \item If ease of use is a significant criterion, StackGres Operator (SPGO) boasts the highest score (82\%), suggesting it as the most user-friendly operator.
    \item If maintenance is a crucial aspect, StackGres Operator obtains the highest scores (100\%), indicating it as the most appropriate operator for this factor.
\end{itemize}

According to the overall results the Crunchy Postgres for Kubernetes is leading with the highest score (68.8\%) followed by CloudNativePG (CNPGO). Nevertheless, it's essential to bear in mind the significance of individual categories and their relevance to the stakeholder's specific needs.

Regarding the security aspect, only a vulnerability analysis has been performed, which does not provide a comprehensive view of security. Furthermore, it remains undetermined what effects these vulnerabilities may have on the cluster, both externally and internally. With this metric in mind, we suggest further research in future studies.

The Percona Operator for PostgreSQL (PPO) cannot be recommended due to its underperformance across all evaluated categories. With the lowest scores across the board, its overall performance falls short in comparison to the other operators, making it the least suitable option based on evaluation criteria.






% ============================================================================ %
