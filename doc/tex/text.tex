% ============================================================================ %
% Encoding: UTF-8 (žluťoučký kůň úpěl ďábelšké ódy)
% ============================================================================ %

% ============================================================================ % 
\nn{Introduction}
It is essential for the database server to be as close as possible to the applications that are using it. This reduces the number of men in the middle between the database and the application, which reduces database access latency and thus reduces overall application latency and increases security. The mass migration of applications to Kubernetes clusters implies a necessary shift of Postgres to Kubernetes. This thesis defines Postgres, Kubernetes, and their Operators. It then further describes the lifecycle of a Postgres cluster, and searches for Operators capable of managing this lifecycle. It establishes metrics by which it tests and evaluates these Operators.  The result of this thesis is the recommendation of a suitable Operator based on the defined metrics.

TBD - remove

The cloud has made our work easier. We no longer have to physically connect new machines to the network, configure network connections, add disks, or even plug in virtual ones. Kubernetes, together with the cloud, can automatically allocate new resources for our applications and, thanks to operators, can even create entire database clusters with high availability. Thanks to operators, it can also automatically set up scaling or backups. It can even restore an entire database system from a backup. This paper is focused on Kubernetes operators for the popular Postgres database management system. Its goal is to find Operators for Postgres. To evaluate their pros and cons. To test them and recommend the best one.

TBD - remove end

% ============================================================================ %
\cast{Theory}

\n{1}{Background}
This chapter introduces the key technologies used in this thesis including Postgres, Kubernetes, and Kubernetes Operators.
\n{2}{Postgres}
TBD: Connect image with text

PostgreSQL is a powerful object-relational database management system (ORDBMS) derived from the POSTGRES package written at the University of California at Berkeley. \cite{docuPgwhatIsPg} \cite{pg14introduction} The first version of POSTGRES was released in June 1989. POSTGRES has been used in many applications, including financial data analysis systems, asteroid tracking databases, medical information database, and several geographic information systems. The size of external community users has nearly doubled by 1993. \cite{docuPgBriefHistory}

POSTGRES was using its POSTQUEL query language from version, until Andrew Yu and Jolly Chen introduced SQL to POSTGRES in 1995. The name has changed to Postgres95. Postgres95 was completely ANSI C code reduced by 25 \% and was 30 – 50 \% faster than Postgres 4.2.  \cite{docuPgBriefHistory}

It was clear by 1996 that the name would not stand the test of time therefore it has been renamed to PostgreSQL. As stated by PostgreSQL documentation \cite{docuPgBriefHistory}: “Many people continue to refer to PostgreSQL as “Postgres” (now rarely in all capital letters) because of tradition or because it is easier to pronounce. This usage is widely accepted as a nickname or alias.“ This thesis will use Postgres as an alias for PostgreSQL as well.

More than 30 years after the first version Postgres has been considered the most used ORDBMS for professional developers by Stack Overflow survey \cite{so2022survey}. According to Riggs and Ciolli \cite{pg14introduction}: “The PostgreSQL feature set attracts serious users who have serious applications. Financial services companies may be PostgreSQL's largest user group, although governments, telecommunication companies, and many other segments are strong users as well.“ It is fully ACID compliant \cite{juba2015learningTransactionIsolation} and supports many kinds of data models such as relational, document, and key/value. \cite{pg14introduction}

\obr{Postgres Architecture \cite{HusseinMediumPostgres} }{fig:Postgres_architecture}{1}{graphics/Postgres_architecture.png}


\n{3}{Write Ahead Log}
Write-ahead Logging (WAL) used by Postgres is a standard technique to ensure data integrity. Its main concept is that changes in data files (where tables and indexes are stored) must only be written after they are logged (saved to a log file). That means the database is updated after the changes are written to disk. In the event of a system crash, all transactions will be recovered from the disk. \cite{docuPgWal}

Although WAL is primarily designed for recovery after a database server crash, its design also allows any changes to the database server state to be replayed backward. A copy of the log is also a form of backup. Thus, for recovery to a point in time, only logs that have been saved to that point in time can be restored. This technique is called Point-In-Time Recovery (PITR). \cite{DocuPgPITR} These log files can also be streamed to other nodes to serve as a replica or remote backup. \cite{pg14replication}

TBD: describe synchronous and asynchronus replication

\n{3}{Backup and restore}
A full set of backup commands is included in Postgres. Among the simple backup commands are pg\_dump and pg\_dumpall, which enable one or more databases to be saved in SQL format. A wide range of configuration options are available for these commands, including compression for large databases or exporting only the database schema. To restore a database from a file at a later time, the psql command can be used, which is capable of restoring a database from its dump. \cite{DocuPgDump} These commands are also helpful with migration from one major Postges version to another because the dumped files are plain SQL commands.

The backup options in Postgres are quite limited. Postgres allows to set up of a backup command that runs after the next log file is created, database dumps, and log streaming. For more advanced backup techniques, additional software such as PgBackRest must be used. \cite{DocuPgPITR}

\n{4}{PgBackRest}
PgBackRest is a reliable and simple backup and restore solution that provides many features on top of classic Postgres backup and restore tools like parallel backup options with compression, local or remote backups, cloud backup (S3. Azure and Google Cloud), or backup encryption. Full, incremental, or differential backup is also supported. \cite{PGbackRest}

TBD: why is it here? Conect to crunhy and operatores


\n{3}{High Availability}
The basic structure of a database cluster consists of one or more database servers, which can be called nodes. In Postgres there are two types of nodes, Primary node and Standby node.  A Primary node is such a node that allows reading and writing information. The newly written information is then streamed to the Standby nodes. Standby nodes are read-only, they do not allow writing. \cite{pg14replication}

Achieving high availability with Postgres is possible by using more than one node in the cluster. Two options are possible here. A single Primary node option, where the Primary node is read and write enabled, and the other nodes are Standby nodes. If the Primary node is unavailable, then the Standby node is promoted to the Primary node. This event is called failover. In this variant, the Primary node streams the logs to the Standby nodes. The second option is to use multiple Primary nodes. However, conflicts can occur because all Primary nodes allow concurrent writes. \cite{docuPgHA}
\n{4}{Patroni}
Since Postgres does not provide any software that can detect that a node is unavailable, it is necessary to use software outside of Postgres \cite{docuPgFailover}, such as Patroni.
Patroni is a popular open-source tool created by Zalando to achieve high availability of Postgres clusters. Patroni uses a distributed configuration source such as ZooKeeper, Etcd, Consul, or Kubernetes for its operation. Patroni can automatically adjust the settings of all managed nodes, therefore it can automate failover and make it seamless. \cite{PalarkMigratingPg} \cite{PatroniDocu}


\n{3}{Load Balancing and Connection Pooling}
Using more than one node allows to direct traffic to a node that is less busy and thus achieve load balancing. Postgres doesn't come with any software that allows splitting the load on different nodes, so it is necessary to use an external load balancer such as HA Proxy or pgBouncer. The load balancer then acts as an intermediary between the database and the client and directs the traffic to the available nodes according to the set rules. These load balancers also enable connection pooling which is a technique for managing and reusing database connections to increase performance and reduce overhead. Connection pooling involves creating a pool of pre-created connections that can be shared and reused by multiple client requests, instead of creating a new connection for each request. This removes the overhead of creating a new process each time a client connects to Postgres and allows the client to use resources that would otherwise be used to service multiple requests (or complete them faster). \cite{PerconaBlogConnectionPooling}

\n{2}{Kubernetes}
Kubernetes, also known as K8s, is an open-source platform for automating deployment, scaling, and management of containerized applications. It provides a way to manage and orchestrate containers, which are units of software that package up an application and its dependencies into a single, isolated package that can run consistently on any infrastructure. \cite{vayghan2019Kubernetes}

As described by Kubernetes Documentation \cite{docuKubeComponents} Kubernetes provides several key features, including:
\begin{itemize}
  \item \textbf{Service discovery:} A container can be exposed by Kubernetes either through its DNS name or its own IP address.
  \item \textbf{Load balancing:} In the case of high traffic to a container, stability of the deployment can be ensured by Kubernetes load balancing and distributing the network traffic.
  \item \textbf{Storage Orchestration:} Storage orchestration in Kubernetes allows for the automatic mounting of a storage system of choice, including local storage, public cloud providers, and others.
  \item \textbf{Automated rollouts and rollbacks:} The desired state of deployed containers can be described using Kubernetes, and the actual state can be changed to the desired state at a controlled rate. For instance, the automation of Kubernetes can be utilized to create new containers for the deployment, remove existing containers, and transfer all their resources to the newly created container.
  \item \textbf{Automatic bin packing:} A cluster of nodes for running containerized tasks is provided to Kubernetes. The amount of CPU and memory required by each container is specified to Kubernetes. The optimal utilization of resources can be achieved by Kubernetes fitting the containers onto the nodes.
  \item \textbf{Self healing:} Containers that fail are restarted by Kubernetes, those that do not respond to the user-defined health check are replaced or killed, and they are not advertised to clients until they are deemed ready to serve.
  \item \textbf{Secret and configuration management:} Sensitive information, such as passwords, OAuth tokens, and SSH keys, can be stored and managed by Kubernetes. The deployment and updating of secrets and application configuration can be done without the need to rebuild container images and without the exposure of secrets in the stack configuration.
\end{itemize}
\n{3}{Kubernetes Components}
Kubernetes cluster is composed of a set of worker machines that run containerized applications called nodes. Each cluster must have at least one node. \cite{docuKubeComponents}
\obr{The components of a Kubernetes cluster \cite{docuKubeComponents}}{}{1}{graphics/Kubernetes_cluster_components.png}

The Kubernetes control plane is the management system of a Kubernetes cluster, responsible for maintaining the desired state of the cluster. It consists of multiple components that work together to manage the cluster and its resources, including pods, services, and volumes. The key components of control plane are \cite{masteringKubernetesConcepts}:
\begin{itemize}
  \item \textbf{kube-APIserver:} Acts as the front-end for the Kubernetes API and exposes the API to other components. \cite{docuKubeComponents}
  \item \textbf{Etcd:} Highly available distributed key-value store that serves as the backing store for the cluster's configuration data. \cite{Dobies2020}
  \item \textbf{kube-scheduler:} Assigns work to nodes in the cluster, such as scheduling pods to run on nodes. \cite{kubeUpAndRunningPods}
  \item \textbf{kube-controller-manager:} Monitors the cluster's state and makes adjustments as necessary to maintain the desired state. \cite{masteringKubernetesConcepts}
  \item \textbf{cloud-controller-manager:} Manages cloud-related tasks such as node creation and management, volume management, and load balancing, allowing the other components of the control plane to focus on their specific responsibilities. Cloud manager is optional. Can be avoided when Kubernetes not used in cloud. \cite{docuKubeComponents}
\end{itemize}
\textbf{Node components:}
Node components in a Kubernetes cluster run on each node and provide crucial functionality for the operation of containers on that node. \cite{docuKubeComponents}
\begin{itemize}
  \item \textbf{kubelet:} Is responsible for communicating with the control plane and ensuring that containers are running and healthy. \cite{kubeUpAndRunning}
  \item \textbf{kube-proxy:} Is responsible for maintaining network rules on the nodes, allowing network communication to the containers. It enables the containers in a pod to communicate with other containers and the outside world, and performs tasks such as load balancing and traffic routing. \cite{kubeUpAndRunning}
  \item \textbf{container runtime:} Is responsible for running containers. \cite{docuKubeComponents}
\end{itemize}

\n{3}{Kubernetes Concepts}
\textcolor{cyan}{RepliceSet extension - Operators p. 28 (Replica is general and appllication agnostic) }

Pod is the smallest deployable unit that can be created in Kubernetes. \cite{docuKubePods} A Pod in Kubernetes is comprised of multiple containers and storage volumes that are run together within the same execution environment. As a result, all containers included in a single Pod will always run on the same machine. \cite{kubeUpAndRunningPods}
A Pod's specifications are outlined in a Pod manifest, which is simply a JSON or YAML text file that represents the Kubernetes API object. Kubernetes follows a declarative configuration approach, where the system's desired state is defined in a configuration file, and the service then implements the necessary changes to make the desired state a reality. \cite{docuKubeStaticPod}

ReplicaSet’s purpose is to ensure a consistent number of replica Pods are running at all times. It is commonly used to guarantee a specified number of identical Pods are available. However, a Deployment is a more advanced concept that oversees ReplicaSets and provides a more streamlined way to make updates to Pods. It also offers additional features. As a result, it's advisable to use Deployments instead of directly utilizing ReplicaSets, unless you have specific update requirements or don't need to make updates at all. \cite{docuKubeReplicaset}

Service is an abstraction layer and defines a group of Pods and the method to access them (often referred to as a micro-service). The group of Pods targeted by a Service is usually specified through a selector. The Service abstraction makes this possible by enabling the decoupling of components. \cite{docuKubeSevice} Kubernetes includes built-in service discovery mechanisms. When a service is created in Kubernetes, it is automatically assigned an IP address and DNS name. Clients and other services can use this name or address to access the service within the Kubernetes cluster. \cite{docuKubeSevice}

Containers and pods in Kubernetes are ephemeral. When a container is terminated, any data it has written to its own filesystem is lost. In Kubernetes, storage is represented by a basic abstraction called "volumes". Containers use these volumes by binding them to their respective pods, and can then access the storage regardless of its physical location as if it were a part of their local filesystem. \cite{masteringKubernetesStorage}

Kubernetes version 1.5 came with a new object called StatefulSet that allows a set of stateful pods to be deployed and managed. Each pod has a unique, stable network identity and a persistent storage volume. This enables stateful applications like databases to be run on Kubernetes. Advantages of using StatefulSets include predictable naming schemes, ordered pod creation and deletion, and unique persistent storage. \cite{docuKubeStatefulSet} \cite{githubKube15}

In version 1.7, Kubernetes introduced the Custom Resources extension to its API. \cite{githubIBMCr} This extension allows Kubernetes to use user-defined resources that are not native to Kubernetes as if they were native. \cite{NewStackCRDs} Custom resources (CR) is an extension to the Kubernetes API that extends the deployment with additional parameters that are not part of it. CR stores these parameters and allows the API server to access them just like the native Kubernetes parts. CR is created in the Kubernetes cluster using a definition called Custom Resource Definition (CRD). \cite{OperatorsAtK8sIface}

Kuberentes controllers are control loops\footnote[1]{A control loop is a process that continuously monitors the state of a system, compares it to a desired state, and makes adjustments to bring the system closer to the desired state.} that constantly check the state of their controlled objects. If the controlled objects are not in the desired state, the controller performs actions to get the controlled objects into that state. For example, restart a crashed node, add a new replica, modify settings, etc. \cite{docuKubeControllers}
However, to work with CR, custom controllers that can work with these resources must be created, these controllers are called Custom Controllers. \cite{docuKubeCR}

\obr{Kubernetes controller \cite{OperatorWhitepaper} }{}{1}{graphics/Kubernetes_controller.png}

\textcolor{red}{TBD - show that Kubernetes can run stateless very well, maybe from Operator book}


\textcolor{cyan}{TBD - Read https://containerjournal.com/kubecon-cnc-eu-2022/why-run-postgres-in-Kubernetes/}

\textcolor{cyan}{TBD - Read data on Kubernetes https://dok.community/dokc-2021-report/}


\n{2}{Running Postgres in Kubernetes}
Kubernetes cannot know all complex stateful applications, which can contain a large number of nodes and have a wide range of uses while remaining general-purpose. The goal of Kubernetes is to provide an abstraction covering basic application concepts and providing options for extensions for more complex applications and their specific operations. Kubernetes cannot and should not know all the possible settings and operations that, for example, a Postgres cluster needs to run. \cite{OperatorsTeaches}

The easiest way to run Postgres in Kubernetes is through the StatefulSet just mentioned. This StatefulSet can start a Postgres pod, create a persistent volume, and connect this volume to the pod. A stateful set can do this for all replicas set in its configuration. It can also scale up or down. Unfortunately, however, all independent Postgres instances created by StatefulSet controller are not synchronized in any manner.

This basic setup may be sufficient for running a single node, but it is no longer sufficient fro managing the whole Postgres lifecycle. For managing whole Postgres lifecycle it is necessary to install other applications in the Kubernetes cluster and then configure the entire Postgres cluster to work with them. This represents a large amount of work and subsequent maintenance that Kubernetes Operators can facilitate.

\n{2}{Database System Lifecycle}
The database system itself is a software like any other. It is therefore also subject to the same life cycle as software.
As depicted in figure \ref{fig:applifecycle} application lifecycle consists of three main parts. It is the governance part, development, and operations. For this thesis, only the operations part is relevant because it is the only part we are able to control.
\obr{Application Life Cycle \cite{ALM}}{fig:applifecycle}{1}{graphics/aplication-lifecycle.png}

Operation is the process of running and managing the application, which starts with deployment and continues until the application is taken out of service. This aspect of the application lifecycle management covers the release of the application into production, ongoing monitoring, and other related tasks. \cite{ALM}

Based on this knowledge, the complete database life cycle is defined as follows.
\begin{itemize}
  \item System installation
  \item System upgrade to a newer version
  \item System backup
  \item System restore
  \item System monitoring
  \item System scaling
  \item System configuration update
  \item System uninstall
\end{itemize}

TBD: connect to capability levels of Operator (define related tasks, chose the capability that would suit them all)

\pagebreak
\n{2}{Operators}
As described in the previous chapters, Kubernetes can run stateless applications very well. But its general purpose makes running complex stateful applications on top of it quite challenging.


This has changed in 2016 when CoreOS came up with Operators as a way to deploy complex applications with state such as databases, caches, or monitoring systems. \cite{IArchiveCOSOperators}

An operator is a special kind of software that extends the Kubernetes API and has a particular knowledge of managed resource that Kubernetes does not have. The Operator also serves as a packaging mechanism for distributing applications including their dependencies in Kubernetes. The Operator can manage, restore, update or monitor the resource. It can also manage very complex applications. The Kubernetes Operator thus replaces the human operator after which it is named, who would otherwise take care of these tasks. \cite{OperatorsPreface} \cite{IArchiveCOSOperators}

\obr{Definition of Kubernetes Operator \cite{IArchiveCOSOperators}}{}{1}{graphics/coreos_operator.png}

CoreOS demonstrated the use of its Operator on Etcd (described in the Kubernetes Components chapter). When new Etcd nodes are created, it is necessary to give them a DNS names and use the Etcd cluster management tools to add the new nodes to an existing cluster. CoreOS has automated these tasks with the Etcd Operator so that all that is required is to increase the number of replicas in the Operator CRD and the Etcd Operator will perform these tasks instead of a human operator. \cite{IArchiveCOSOperators}
By embedding the human operator's operational knowledge into the code, this ensures that these tasks are repeatable, testable and upgradable. It also ensures that the necessary operations are always performed, executed in the order in which they are supposed to be performed, and none are skipped. This reduces the number of hours spent on dull but essential work such as backups. \cite{OperatorWhitepaper}

As described by Operator White Paper \cite{OperatorWhitepaper} and depicted in figure \ref{fig:operatorPatern}, Operator consists of the following parts
\begin{itemize}
  \item The managed application or infrastructure
  \item Software that has some specific knowledge of the managed application or infrastructure and allows the user to declaratively set the desired state
  \item Custom Controller, which is responsible for achieving the desired state
\end{itemize}

\obr{Operator pattern \cite{OperatorWhitepaper} }{fig:operatorPatern}{1}{graphics/operator_patern.png}

Like human operators, Kubernetes Operators can have a level of manual skill ranging from basic software installation and setup skills to a high level where they can scale software vertically or horizontally to automatically change the configuration or detect abnormalities. All Operator maturity levels are depicted in the figure \ref{fig:operatorCapabilities}. The highest level can only be reached by programming the Operator in the GO programming language or by using the Ansible automation tool. \cite{OperatorsOframework}

\obr{Operator maturity levels described by Operator Framework \cite{OFrameworkMaturity}}{fig:operatorCapabilities}{1}{graphics/operator_capabilities.png}

As stated in the Operator white paper, \cite{OperatorWhitepaper} the Operator should be able to cover the complete life cycle of the managed resource as defined in the previous chapter without the need for external installation or upgrade intervention. Specifically as follows:
\begin{itemize}
  \item Install or take ownership of the controlled application.
  \item Upgrade the managed application, including the monitoring of the upgrade process. It should also be able to roll back in case of failure. He should record the status of the upgrade.
  \item Back up the managed application and log when the application was last backed up and the status of that backup.
  \item Restore the application from the backup.
  \item Provide monitoring of the managed application.
  \item Scale the application.
  \item Automatically adapt the configuration of the application.
  \item Uninstall or disconnect from the application.
\end{itemize}

These are all capabilities that an Operator should have at the highest level No. 5 - Autopilot. For lifecycle management described in previous chapter, the minimum level of Operator capabilities must be at least level No. 4 - Deep Insights with an option to scale.

The Kubernetes cluster is divided into individual namespaces that separate the objects and names in the cluster and can have constraints applied to them. This partitioning makes it easier to share the cluster between users or entire teams. The object name must be unique within a namespace, but not between namespaces.  An operator usually operates in its own namespace so it has a Namespace Scope, but it can also operate in the whole cluster in which case it will be a Cluster Scope Operator. Namespace Scope Operators are more flexible and easier to upgrade due to their independence from the rest of the cluster. Operator rights are further restricted by the so-called Role-Based Acceess Control (RBAC), which grants the rights assigned to the Operator. \cite{ OperatorsAtK8sIface}

The following options are advised by the Operator white paper \cite{OperatorWhitepaper} in case the Operator is to be used for controlling the resource:
\begin{itemize}
  \item	Consultation with the creator of the resource to be controlled about the possibilities of using the Operator.
  \item	The search for public Operator registries that provide a platform for publishing Operators and the underlying documentation.
  \item	The creation of own Operator.
\end{itemize}


\n{1}{Search for Postgres Operators}
TBD: each operator security Artifact hub https://artifacthub.io/packages/olm/community-operators/postgresql

TBD: Postgres update every three months https://access.crunchydata.com/documentation/postgres-operator/5.3.0/tutorial/update-cluster/

TBD: Container types CloudNativePG is built on immutable application containers. What does it mean? https://cloudnative-pg.io/documentation/1.19/faq/

As recommended in the previous section. The choice of Operator should first be consulted with the manufacturer of the controlled source. Postgres offers the following Kubernetes Operators in its software catalog \cite{docuPgSwCatalogue}: CloudNativePG, EDB Postgres for Kubernetes a Kubegres.

The next step involved a search of the Operators' registers. In particular the Operator Hub. \cite{OperatorHubPGSearch} Operator Hub presents nine operators with varying levels of capabilities, including Crunchy Postgres for Kubernetes by Crunchy Data, EDB Postgres for Kubernetes by EnterpriseDB Corporation, Ext Postgres Operator by movetokube.com, Percona Operator for PostgreSQL by Percona, Postgres-Operator by Zalando SE, Postgresql Operator by Openlabs, PostgreSQL Operator by Dev4Ddevs.com and StackGres by OnGres.

A deeper internet search revealed Stolon Operator. \cite{PalarkComparingKubernetes}

Of the thirteen operators available, only five meet our minimum capability requirement of Deep Insight, namely: Crunchy Postgres for Kubernetes, EDB Postgres for Kubernetes, Percona Operator for PostgreSQL, CloudNativePG Operator, and StackGres Operator. As a result, only these five will be subjected to deeper research, testing, and evaluation.

\pagebreak
\n{2}{Crunchy Postgres for Kubernetes}
Crunchy Postgres for Kubernetes (PGO) is a Postgres Operator provided by Crunchy Data, which offers a declarative solution for the management of PostgreSQL clusters, with a focus on automation.
Crunchy Data is a company that specializes in providing open-source software solutions for Postgres. The company also provides a range of support, consulting, and training services to help organizations implement and optimize their Postgres deployment. \cite{Crunchy}:

PGO’s capabilities are the following:
\begin{itemize}
  \item \textbf{Postgres Cluster Provisioning:} PGO is able to create \cite{CrunchyDocCreate}, update \cite{CrunchyDocUpdate} or delete Postgres cluster \cite{CrunchyDocDelete}
  \item \textbf{High Availability:} High availability is achieved by adding additional nodes. PGO uses a synchronous replication technique with Primary and Standby architecture. \cite{CrunchyDocHA}
  \item \textbf{Postgres updates:} PGO is able to apply minor patches \cite{CrunchyDocMinorUpdates}, and major upgrades since version 5.1. \cite{CrunchyBlogUpdates}
  \item \textbf{Backups:} PGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob), ad hoc backups, and backup encryption. \cite{CrunchyDocBackups}
  \item \textbf{Disaster Recovery:} PGO is capable of Point In Time recovery, in place Point in Time Recovery, restore of an individual database. \cite{CrunchyDocDisasterRecovery}
  \item \textbf{Cloning:} PGO is able to clone cluster. \cite{CrunchyDocDisasterRecovery}
  \item \textbf{Monitoring:} Monitoring is provided by Prometheus, Grafana, and Alertmanager. \cite{CrunchyDocMonitoring}
  \item \textbf{Connection Pooling:} PgBouncer connection pooler from Postgres is part of PGO. \cite{CrunchyDocConnectionPooling}
  \item   \item \textbf{Customization:} PGO provides a wide area of Postgres customization. \cite{CrunchyDocCustomisation}
\end{itemize}

TBD: Add customization capabilities of PGO

The current stable version of PGO is 5.3.0 was released on 13th January 2023. \cite{ArtifactHubCrunchy}
PGO is compatible with following platforms: Kubernetes 1.22-1.25, OpenShift 4.8-4.11, Rancher, Google Kubernetes Engine (GKE), including Anthos, Amazon EKS, Microsoft AKS, VMware Tanzu. \cite{CrunchyDoc}

PGO is distributed under the Apache License 2.0, an open-source license that allows for both commercial and non-commercial use. With regards to capability, PGO is considered to have the highest capability level, labeled as Autopilot. \cite{OperatorHubCrunchy}

PGO consists of the following key components \cite{CrunchyPGOGit}:
\begin{itemize}
  \item High Availability: Patroni
  \item Backups: PgBackRest
  \item Connection Pooler: PgBouncer
  \item Monitoring: PgMonitor, Prometheus, Grafana, and Alertmanager
\end{itemize}
\obr{PGO’s architecture \cite{CrunchyDocArchitecture}}{}{1}{graphics/pgo_architecture.png}


\pagebreak
\n{2}{EDB Postgres for Kubernetes}
The EDB Postgres for Kubernetes (EDBO) is a fully supported operator that has been designed, developed, and maintained by EnterpriseDB Corporation. It provides comprehensive coverage of the entire lifecycle of highly available PostgreSQL database clusters with a Primary/Standby architecture, utilizing native streaming replication. The operator is based on the open-source CloudNativePG operator and offers additional benefits. \cite{OperatorHubEDB}

EnterpriseDB (EDB) is a software company that provides enterprise-class PostgreSQL software and services. EDB is a leading provider of PostgreSQL technology, offering a range of products and services designed to help organizations adopt, deploy, and manage PostgreSQL databases. \cite{EDB}

As stated previously EDBO is based on open-source CloudNativePG operator. EDBO provides additional features on top of CloudNativePG operator such as support for Oracle compatibility using EDB Postgres Advanced Server, support for additional platforms such as IBM Power, and additional enterprise-grade security features. \cite{EDBdocu}

EDBO is distributed under the EDB Limited Usage License Agreement, a proprietary license that is specific to software provided by EnterpriseDB Corporation. A license key is always required for the operator to work longer than 30 days. \cite{EDBdocuLicence} Due to the restrictive nature of the license EDBO will no longer be subject to testing and evaluation but its key component CloudNativePG will.

\pagebreak

\n{2}{CloudNativePG}
The CloudNativePG operator (CNPGO) is an operator that is available as an open-source solution and aims to manage PostgreSQL workloads across various Kubernetes clusters running in private, public, hybrid, or multi-cloud environments. The Operator aligns with DevOps principles and concepts like immutable infrastructure and declarative configuration. \cite{CNPGdocu}

Initially developed by EDB, CNPGO was later made available to the public as an open-source software under the Apache License 2.0. In April 2022, the project was submitted to CNCF Sandbox for further development and community engagement. \cite{CNPGdocu}

CNPGO’s capabilities are the following:
\begin{itemize}
  \item \textbf{Postgres Cluster Provisioning:} CNPGO is able to create, update or delete Postgres cluster. \cite{CNPGdocuCapabilityLevels}
  \item \textbf{High Availability:} High availability is achieved by adding additional nodes. PGO uses a synchronous replication technique with Primary and Standby architecture. \cite{CNPGdocuReplication}
  \item \textbf{Direct database imports:} CNPGO provides direct database import from remote Postgres server by using pg\_dump and pg\_restore even on different Postgres versions. \cite{CNPGdocuDatabaseImports}
  \item \textbf{Postgres updates:} CNPGO is able to apply minor patches. \cite{CNPGdocuUpdates} Major updates are possible by previously mentioned Direct database imports.
  \item \textbf{Backups:} CNPGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob), on-demand backups, and backup encryption \cite{CNPGdocuBackup}\cite{CNPGdocuTDE}. Due to EDB’s backup software Barman backup compression is available also. \cite{CNPGdocuBackup}
  \item \textbf{Disaster Recovery:} CNPGO is capable of Point In Time recovery. Single database recovery was not mentioned in the documentation. \cite{CNPGdocuBackup}
  \item \textbf{Cloning:} CNPGO is able to create cluster replicas. \cite{CNPGdocuReplication}
  \item \textbf{Monitoring:} Monitoring can be provided by the additional installation of Prometheus, and Grafana, and Alertmanager. \cite{CNPGdocuQuickstart}
  \item \textbf{Connection Pooling:} Is provided by native Postgres pooler PgBouncer. \cite{CNPGdocuConnectionPooling}
  \item \textbf{Customization:} CNPGO provides a wide area of Postgres customization such as max parallel workers tuning or WAL configuration \cite{CNPGdocuConfiguration}
\end{itemize}

The current major stable version of CNPGO is 1.19 was released on 14th February 2023. CNPGO is distributed under the Apache License 2.0 open-source license. CNPGO is considered to have the highest capability level, labeled as Autopilot. \cite{CNPGdocu}

CNPGO consists of the following key components \cite{PostgresOnKubernetes} \cite{CNPGdocuQuickstart}:
\begin{itemize}
  \item High Availability: Postgres instance manager
  \item Backups: Barman
  \item Connection Pooler: PgBouncer
  \item Monitoring: Prometheus, Grafana, and Alertmanager
\end{itemize}

\obr{CNPGO’s architecture \cite{CNPGdocuConnectionPooling}}{}{1}{graphics/CNPGO_architecture.png}

\pagebreak
\n{2}{StackGres Operator}
StackGres (SPGO) is a comprehensive distribution of PostgreSQL for Kubernetes, delivered in a user-friendly deployment package. The distribution includes a set of PostgreSQL components that have been carefully selected and optimized to work seamlessly with each other. \cite{SPGOgitlab}

SPGO is developed by OnGres that was established as a result of years of experience in working with and creating products based on Postgres and supporting clients with their Postgres infrastructures. Postgres databases are at the heart of the company's business, as the name suggests. \cite{OnGres}

SPGO’s capabilities are the following \cite{OnGres}:
\begin{itemize}
  \item \textbf{Postgres Cluster Provisioning:} SPGO is able to create, update or delete Postgres cluster.
  \item \textbf{High Availability:} High availability is achieved by adding additional nodes with Primary and Standby architecture.
  \item \textbf{Postgres updates:} SPGO is able to apply minor patches. Major updates are possible by SGDbOps \cite{SPGODocuMajorUpdates}.
  \item \textbf{Backups:} SPGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob)
  \item \textbf{Disaster Recovery:} SPGO is capable of Point In Time recovery. Database recovery was not mentioned in the documentation.
  \item \textbf{Cloning:} SPGO is able to create cluster replicas.
  \item \textbf{Monitoring:} Monitoring is provided by Prometheus, Grafana, and Alertmanager.
  \item \textbf{Connection Pooling:} Is provided by native Postgres pooler PgBouncer.
  \item \textbf{Customization:} SPGO provides a wide area of Postgres customization such as WAL configuration, archive mode, vacuum, etc. \cite{SPGODocuCustomization}
  \item \textbf{Mamagement Console:} SPGO provides a fully featured management web console.

\end{itemize}

The current stable version of SPGO is 1.4.2 was released on 24th January 2022. \cite{SPGOgitlabChangelog} SPGO is distributed under the AGPL3 open-source license. \cite{SPGODocuLicence}
With regards to capability, SPGO is considered to have the second highest capability level, labeled as Deep Insights. \cite{OperatorHubStackgres}

SPGO consists of the following key components \cite{PostgresOnKubernetes}:
\begin{itemize}
  \item High Availability: Patroni
  \item Backups: WAL-G
  \item Connection Pooler: PgBouncer
  \item Monitoring: Prometheus, Grafana, and Alertmanager.
\end{itemize}


\obr{SPGO’s architecture \cite{SPGOdocuArchitecture}}{}{1}{graphics/SPGO_architecture.png}

\pagebreak
\n{2}{Percona Operator for PostgreSQL}
Percona is a company that provides services and solutions for open-source database technologies. It offers expertise, support, and software for MySQL, MongoDB, and PostgreSQL. The company's offerings help organizations manage their open-source databases and ensure optimal performance, security, and scalability. \cite{Percona}

Percona Operator for PostgreSQL (PPO) is based on Crunchy Postgres for Kubernetes. Percona forked PGO v 4.7 and has added enhancements for monitoring, upgradability, and flexibility. \cite{PerconaBlogProsAndCons}

Differences between PGO and PPO are the following:
\begin{itemize}
  \item \textbf{Postgres updates:} PPO provides automatic Postgres updates for minor and major versions of Postgres.  \cite{PerconaDocuUpdate}
  \item \textbf{Backups:} PPO is not able to back up to Azure. \cite{PerconaDocuCompare} Although it uses Patroni, which has this ability.
  \item \textbf{Disaster Recovery:} PPO documentation does not mention the possibility of restoring a single database from a backup. \cite{PerconaDocuBackups}
  \item \textbf{Monitoring:} PPO is not using the usual monitoring stack consisting of Prometheus and Grafana but their own Percona Monitoring and Management. \cite{PerconaDocuMonitoring}
\end{itemize}

The current stable version of PPO is 1.3.0 was released on 4th August 2022. The upcoming version 2.0.0 is not yet production ready therefore will not be subject to testing and evaluation. \cite{PerconaDocuV2} PPO is distributed under the Apache License 2.0, an open-source license that allows for both commercial and non-commercial use. With regards to capability, PPO is considered to have the second highest capability level, labeled as Deep Insights. \cite{OperatorHubPercona}

PPO consists of the following key components \cite{PostgresOnKubernetes}:
\begin{itemize}
  \item High Availability: Patroni
  \item Backups: PgBackRest
  \item Connection Pooler: PgBouncer
  \item Monitoring: Percona Monitoring and Management
\end{itemize}

\obr{PPO’s architecture \cite{CrunchyDocArchitecture}}{}{1}{graphics/PPO_architecture.png}

\n{2}{Key Differences between selected Operators}

\tab{Table of key differences between selected Operators}{tab:keyDifferences}{1}{|l|l|l|l|l|}{
  \hline
  & PGO 5.3.0 & CNPGO 1.18.1 & SPGO 1.4.2 & PPO 1.3.0\\ \hline
  \vtop{\hbox{\strut Programming}\hbox{\strut language}} & GO & GO & JAVA & GO\\ \hline
  \vtop{\hbox{\strut Supported}\hbox{\strut Postgres}}  & 12, 13, 14, 15 & 11, 12, 13, 14, 15 & 12, 13, 14, 15 & 12, 13, 14\\ \hline
  \vtop{\hbox{\strut Major version}\hbox{\strut updates}} & Manual & Manual & Manual & Automatic\\ \hline
  \vtop{\hbox{\strut Single database}\hbox{\strut restore}} & Yes & No & No & No\\ \hline
  Licence & Apache 2.0 & Apache 2.0 & Apache 2.0 & AGPL 3\\ \hline
  User interface &  No & No & Yes & No\\ \hline
  Capability Level & Autopilot & Autopilot & Deep Insight & Deep Insights\\ \hline
}

\n{1}{Testing methodology}
As mentioned in the previous chapters, Operators will be subjected to tests to manage the complete application lifecycle.

Subsequently:
\begin{itemize}
  \item Operator install
        \begin{itemize}
          \item Installation of the Operator in Kubernetes cluster
          \item Check if Operator was installed
        \end{itemize}
  \item Cluster install
        \begin{itemize}
          \item Installation of the cluster with an older version of Postgres and two nodes, one as Primary, the other as Standby
          \item Check if all components were installed successfully
        \end{itemize}
  \item Cluster accepts connections
        \begin{itemize}
          \item Connection to cluster
          \item Check if connection was successful
        \end{itemize}
  \item Cluster synchronization
        \begin{itemize}
          \item Creation of database on primary node
          \item Check if database created in previous step was also created on Standby node
        \end{itemize}
  \item Cluster update to a newer version
        \begin{itemize}
          \item Update the cluster to a newer version
          \item Update result check
        \end{itemize}
  \item Recovery of the Primary node after a crash
        \begin{itemize}
          \item Delete of the primary node
          \item Failover check
          \item New Standby creation check
        \end{itemize}
  \item Cluster backup
        \begin{itemize}
          \item Creation of backup volume
          \item Postgres cluster backup to backup volume
          \item Backup result check
        \end{itemize}
  \item Cluster restore from backup
        \begin{itemize}
          \item Restoration of the cluster from the backup created in the previous step
          \item Restored cluster check
        \end{itemize}
  \item Cluster vertical scaling
        \begin{itemize}
          \item Addition of a node to the cluster
          \item Number of nodes check
          \item Removal of the added node
          \item Number of nodes check
        \end{itemize}
  \item Cluster configuration update
        \begin{itemize}
          \item Change the cluster configuration
          \item TBD - figure out what we are going to change and justify it
          \item Implementation of change check
        \end{itemize}
  \item Cluster monitoring
        \begin{itemize}
          \item Installation of monitoring
          \item Visual monitoring check
        \end{itemize}
  \item Cluster uninstall
        \begin{itemize}
          \item Removal of the backup volume
          \item Removal of the cluster
          \item Removal result check
        \end{itemize}
  \item Operator uninstall
        \begin{itemize}
          \item Operator uninstall
          \item Check if Operator was removed with all its dependencies
        \end{itemize}
\end{itemize}


TBD: study what the methodology is, figure out how to rate each point








%\obr{2022 Developer Survey \cite{so2022survey}}{}{1}{graphics/postgres_stack_overflow_survey.png}


\cast{Praktická část}
\n{1}{Nadpis první kapitoly praktické části}
TBD: Describe architecture

TBD: Describe kind cluster (maybe all in theory)

TBD: Describe kubectl

TBD: First create manual testing and describe it, then automate these test with Kubernetes E2E tests in GO

The repository of this thesis contains a folder of tests in which the individual test suites are located. Each operator has a folder (test suite) that contains individual test cases numbered according to the order of execution, followed by a Makefile file that lists the specific commands that are executed to test the Operator.

TBD: describe isntalled dependencies: kubectl, psql, jq, busybox

\pagebreak
\n{2}{Crunchy Postgres for Kubernetes (PGO) functional tests}
The Crunchy documentation includes a Quickstart chapter\footnote{https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart} that describes examples of Operator installation and usage. This chapter also contains a link to a repository with yaml manifests\footnote{https://github.com/CrunchyData/postgres-operator-examples} that allow kubectl to install, modify or delete Operator and managed resources in a kubernetes cluster. The manifests are distributed under the Apache 2.0 license and are also part of the repository of this thesis\footnote{https://github.com/Ovec/Bachelors-thesis}. In case of a any modification of these manifests, the reason for the modification and the modification itself is described.



\n{3}{Operator install}
The installation of the operator was done with the following commands in install directory\footnote{/tests/PGO/1\_operator\_install}, that contains yaml manifests extracted from the PGO examples repository without any change.

\textcolor{cyan}{this will install pgo version 5.3.1}

\begin{verbatim}
kubectl apply -k ./namespace
kubectl apply --server-side -k ./default
\end{verbatim}

These commands created the following kubernetes objects:
\begin{itemize}
  \item Operator's Custom resource definition named \\postgresclusters.postgres-operator.crunchydata.com
  \item Operators service accounts named pgo and postgres-operator-upgrade
  \item Cluster roles postgres-operator
  \item Cluster role postgres-operator-update
  \item A cluster role binding named postgres-operator that links service account pgo to the cluster role postgres-operator
  \item Cluster role binding named postgres-operator-upgrade which links service account postgres-operator-upgrade to cluster role postgres-operator-upgrade
  \item Deployment pgo
  \item Deployment pgo-upgrade
\end{itemize}


To check that the Operator is created correctly, the following command was used:
\begin{verbatim}
kubectl -n postgres-operator get pods
\end{verbatim}

This command displays all pods from the postgres-operator namespace. If the Operator was installed correctly the command will display the pod operator with the status Running.

The result of this command was the following:
\begin{verbatim}
NAME                           READY   STATUS    RESTARTS
pgo-864ddbd4cf-4pcd6           1/1     Running   0
pgo-upgrade-78fc8d8f94-jbnrl   1/1     Running   0 
\end{verbatim}

Therefore, it can be stated that the Operator has been installed and has successfully passed this test case.
\n{3}{Cluster install with older major Postgres version}
\label{chap:pgoclusterinstall}
By using the kubectl describe command on the operator pod, the supported Postgres docker image, pgAdmin, pgBackRest, pgBouncer and pgExporter can be displayed. The result for the deployed Operator is the following\footnote{'registry.developers.crunchydata.com/crunchydata/' is being replaced with '<registry>'}:

\begin{verbatim}
PGO_NAMESPACE:                      
postgres-operator (v1:metadata.namespace)                                    
CRUNCHY_DEBUG:                      
true                                  
RELATED_IMAGE_POSTGRES_13:          
<registry>crunchy-postgres:ubi8-13.10-0
RELATED_IMAGE_POSTGRES_13_GIS_3.0:  
<registry>crunchy-postgres-gis:ubi8-13.10-3.0-0                               
RELATED_IMAGE_POSTGRES_13_GIS_3.1:  
<registry>crunchy-postgres-gis:ubi8-13.10-3.1-0                               
RELATED_IMAGE_POSTGRES_14:          
<registry>crunchy-postgres:ubi8-14.7-0 
RELATED_IMAGE_POSTGRES_14_GIS_3.1:  
<registry>crunchy-postgres-gis:ubi8-14.7-3.1-0                                
RELATED_IMAGE_POSTGRES_14_GIS_3.2:  
<registry>crunchy-postgres-gis:ubi8-14.7-3.2-0                                
RELATED_IMAGE_POSTGRES_14_GIS_3.3:  
<registry>crunchy-postgres-gis:ubi8-14.7-3.3-0                                
RELATED_IMAGE_POSTGRES_15:          
<registry>crunchy-postgres:ubi8-15.2-0 
RELATED_IMAGE_POSTGRES_15_GIS_3.3:  
<registry>crunchy-postgres-gis:ubi8-15.2-3.3-0                                
RELATED_IMAGE_PGADMIN:              
<registry>crunchy-pgadmin4:ubi8-4.30-10
RELATED_IMAGE_PGBACKREST:           
<registry>crunchy-pgbackrest:ubi8-2.41-4                                      
RELATED_IMAGE_PGBOUNCER:            
<registry>crunchy-pgbouncer:ubi8-1.18-0
RELATED_IMAGE_PGEXPORTER:           
<registry>crunchy-postgres-exporter:ubi8-5.3.1-0
\end{verbatim}

Since the yaml manifest provided contains Postgres version 14, but the list of supported versions includes version 15, the manifests will be used as is and Postgres will be updated from version 14 to version 15 in the Postgres version update test case.

To install the Postgres cluster, the following command must be executed in the install directory\footnote{/tests/PGO/1\_cluster\_install}.
\begin{verbatim}
kubectl apply -k ./high-availability
\end{verbatim}

This command has installed the following pods and their services in postgres-operator namespace

\begin{verbatim}
NAME                                  READY   STATUS      RESTARTS
hippo-ha-backup-2bc7-rv8s9            0/1     Completed   0
hippo-ha-pgbouncer-7c6fcd4668-9fn7v   2/2     Running     0
hippo-ha-pgbouncer-7c6fcd4668-jsvmj   2/2     Running     0
hippo-ha-pgha1-nmq4-0                 4/4     Running     0
hippo-ha-pgha1-pmsk-0                 4/4     Running     0
hippo-ha-repo-host-0                  2/2     Running     0
\end{verbatim}

\begin{verbatim}
NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)
hippo-ha-ha          ClusterIP   10.96.225.2    <none>        5432/TCP
hippo-ha-ha-config   ClusterIP   None           <none>        <none>
hippo-ha-pgbouncer   ClusterIP   10.96.49.246   <none>        5432/TCP
hippo-ha-pods        ClusterIP   None           <none>        <none>
hippo-ha-primary     ClusterIP   None           <none>        5432/TCP
hippo-ha-replicas    ClusterIP   10.96.74.192   <none>        5432/TCP
\end{verbatim}

Using kubectl describe, it was found that the hippo-ha-pgha pods contain four containers including a Postgres server, a pgBackrest server for backups, Patroni for replica synchronization, and replication-cert. Hippo-ha-pgbouncer pods contain pgBouncer and pgBouncer-config containers taking care of connection pooling for hippo-ha-pgha pods. Hippo-ha-repo-host-0 contains the pgBackrest server and pgBackrest-config containers from the yaml manifest it was found that it created a volume and backed up the database server when it was created.

Using kubectl describe, it was found that the service hippo-ha-ha is connected to the pod hippo-ha-pgha1-pmsk-0, the service hippo-ha-pgbouncer is connected to the pod pgBouncer, the service hippo-ha-primary is connected to the service hippo-ha-ha, and hippo-ha-replicas is connected to the pod hippo-ha-pgha1-nmq4-0. It can thus be concluded that there should be a primary replica under hippo-ha-pgha1-pmsk-0 and a standby replica under hippo-ha-pgha1-nmq4-0. This finding will be verified in Chapter ~\ref{chap:pgosync}

All components were successfully installed and Kubernetes did not report any errors so it can be said that Operator passed this test successfully.
\n{3}{Cluster accepts connections}
Since the Postgres cluster is not exposed outside the cluster, it was necessary to install some utility in the cluster that will have telnet which can connect to port 5432 of the cluster, like BusyBox.

Yaml manifest of the installed BusyBox.
\begin{verbatim}
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    - name: busybox
      image: busybox
      command: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
\end{verbatim}

Busybox was installed with the following command executed in test directory\footnote{/tests/PGO/3\_cluster\_accepts\_connections}.
\begin{verbatim}
kubectl apply -f ./busybox.yaml
\end{verbatim}

The connection to the cluster was successfully established with the following command:

\begin{verbatim}
kubectl exec busybox -- telnet \
$(kubectl get service hippo-ha-pgbouncer -n postgres-operator \
-o jsonpath='{.spec.clusterIP}') 5432
\end{verbatim}

Therefore, it can be said that the cluster is receiving the connection and has passed this test successfully.

\n{3}{Cluster synchronization}
\label{chap:pgosync}
In order to find out if the nodes are synchronized, a test database will be created on the Primary node and then the name of this database will be selected from the list of databases on Standby node.

This was achieved by the following commands. The first command created the database on the primary node and the second checked for its existence on the standy. The commands contain subcommands that return the name of the primary or standby node depending on the set selector.

\begin{verbatim}
kubectl exec $$(kubectl -n postgres-operator get pods \
--selector=postgres-operator.crunchydata.com/role=master \
-o jsonpath='{.items[*].metadata.name}') \
-n postgres-operator \
-- psql -U postgres -c "create database test"
\end{verbatim}
\begin{verbatim}
kubectl exec $$(kubectl -n postgres-operator get pods \
--selector=postgres-operator.crunchydata.com/role=replica \
-o jsonpath='{.items[*].metadata.name}') \
-n postgres-operator \
-- psql -U postgres -c "SELECT 1 as success FROM 
pg_database WHERE datname = 'test'"
\end{verbatim}

\n{3}{Cluster update to a newer version}
Updating a major version of the Postgres database server using the PGO Operator consists of the following steps. The first step must be installed using kubectl pgUpgrade which manifest is depicted below.
\begin{verbatim}
kubectl apply -k ./pg-upgrade
\end{verbatim}

Additionally, the allow update annotation must be added to the hippo-ha cluster using the following command.
\begin{verbatim}
kubectl -n postgres-operator annotate postgrescluster hippo-ha \
postgres-operator.crunchydata.com/allow-upgrade="hippo-upgrade"
\end{verbatim}


Then the cluster must be shut down by adding the spec parameter shutdown set to true in the same yaml manifest under which the cluster was created and then applied using the same command as the cluster was created.

Using the command that lists the status of pgUpgrade\footnote{kubectl describe pgupgrade hippo-upgrade -n postgres-operator}, upgrade status can be monitored. The condition section lists the status of the upgrade, it looks similar to the one below.

\begin{verbatim}
Last Transition Time:  2023-03-24T16:40:13Z
Message:               Upgrade progressing for cluster hippo-ha
Observed Generation:   1
Reason:                PGUpgradeProgressing
Status:                True
Type:                  Progressing
\end{verbatim}
\begin{verbatim}
Last Transition Time:  2023-03-24T16:46:39Z
Message:               PostgresCluster hippo-ha is running version 15
Observed Generation:   1
Reason:                PGUpgradeCompleted
Status:                False
Type:                  Progressing
\end{verbatim}
\begin{verbatim}
Last Transition Time:  2023-03-24T16:46:39Z
Message:               PostgresCluster hippo-ha is ready 
                       to complete upgrade to version 15
Observed Generation:   1
Reason:                PGUpgradeSucceeded
Status:                True
Type:                  Succeeded
\end{verbatim}

Once the conditions status is Succeeded, the upgrade is complete. Then the manifest still needs to be changed again, setting the spec shutdown parameter to false, the postgresVersion parameter to 15 and the container image to the supported version shown above in Chapter~\ref{chap:pgoclusterinstall}.

The cluster was successfully upgraded from version 14.7 to version 15.2 by this process and contained a test database. Therefore, Operator passed this test successfully.

\n{3}{Recovery of the Primary node after a crash}


\n{3}{Cluster backup}
\n{3}{Cluster restore from backup}
\n{3}{Cluster vertical scaling}
\n{3}{Cluster configuration update}
\n{3}{Cluster monitoring}
- monitoring pgo provides monitoring but you have to install it

- you must also add PGO exporter in order to export metrics to Prometheus

- Pgo provides five Grafana dashboards. POD details fig. \ref{fig:PGO_monitoring_POD_details} (various pod statistics for each pod, CPU, memory, network and disk usage for each POD),
PostgreSQL service health fig. \ref{fig:PGO_monitoring_postgresSQL_health} (Saturation, Traffic, Errors, Query durration),
PgBackRest dashboard fig. \ref{fig:PGO_monitoring_pgBackRest} (Information about backups),
PostgreSQL details \ref{fig:PGO_monitoring_postgresSQLdetails} (complete cluster overview including active conncetions, cache hit ratio, database size, etc.),
and Query statistics \ref{fig:PGO_monitoring_QueryStatistics} (verry helpfull statistic for each Postgres operator, shows query mean time, query run time, and highlites slow queries)

- After installing Prometheus exporter everything worked flawlessly, cluster was discovered by Prometheus and appeared in Grafana

\obr{PGO monitoring POD details}{fig:PGO_monitoring_POD_details}{1}{graphics/PGO_monitoring_POD_details.png}
\obr{PGO monitoring PostgreSQL health}{fig:PGO_monitoring_postgresSQL_health}{1}{graphics/PGO_monitoring_postgresSQL_health.png}
\obr{PGO monitoring PgBackRest}{fig:PGO_monitoring_pgBackRest}{1}{graphics/PGO_monitoring_pgBackRest.png}
\obr{PGO monitoring PostgreSQL details}{fig:PGO_monitoring_postgresSQLdetails}{1}{graphics/PGO_monitoring_postgresSQLdetails.png}
\obr{PGO monitoring Query Statistics}{fig:PGO_monitoring_QueryStatistics}{1}{graphics/PGO_monitoring_QueryStatistics.png}


\n{3}{Cluster uninstall}



\n{3}{Operator uninstall}






\n{1}{Nadpisy a podnadpisy}
Na této stránce je k vidění způsob tvorby různých úrovní nadpisů.

\n{2}{Podnadpis A}
Text

\n{2}{Podnadpis B}
Text

\n{2}{Podnadpis C}
Text

\n{3}{Podpodnadpis alfa}
Text

\n{3}{Podpodnadpis beta}
Text

\n{3}{Podpodnadpis gama}
Text

\n{2}{Podnadpis D}
Text


\n{1}{Vkládání obrázků, tabulek a citací}
Níže následují ukázky vložení obrázku, tabulky a různorodých citací.


\n{2}{Obrázek}
Obrázek \ref{fig:logo} prezentuje logo Fakulty aplikované informatiky.

% Obrázek lze vkládat pomocí následujícího zjednodušeného stylu, nebo klasickým LaTex způsobem
% Pozor! Obrázek nesmí obsahovat alfa kanál (průhlednost). Jde to totiž proti požadovanému standardu PDF/A.
\obr{Popisek obrázku}{fig:logo}{0.5}{graphics/logo/fai_logo_cz.png}


\n{2}{Tabulka}
Tabulka \ref{tab:priklad} obsahuje dva řádky a celkem 7 sloupců.

% Tabulku lze vkládat pomocí následujícího zjednodušeného stylu, nebo klasickým LaTex způsobem
\tab{Popisek tabulky}{tab:priklad}{0.65}{|l|c|c|c|c|c|r|}{
  \hline
  & 1 & 2 & 3 & 4 & 5 & Cena [Kč] \\ \hline
  \emph{F} & (jedna) & (dva) & (tři) & (čtyři) & (pět) & 300 \\ \hline
}

% ============================================================================ %

% Pokud Vaše práce obsahuje analytickou část, stačí odkomentovat nasledujících dva řádky
%\cast{Analytická část}
%\n{1}{Nadpis}


% ============================================================================ %

% ============================================================================ %
\nn{Závěr}
Text závěru.




% ============================================================================ %
