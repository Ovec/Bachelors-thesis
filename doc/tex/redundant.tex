PGO’s capabilities are the following:
\begin{itemize}
    \item \textbf{Postgres Cluster Provisioning:} PGO is able to create \cite{CrunchyDocCreate}, update \cite{CrunchyDocUpdate} or delete Postgres cluster \cite{CrunchyDocDelete}
    \item \textbf{High Availability:} High availability is achieved by adding additional nodes. PGO uses a synchronous replication technique with Primary and Standby architecture. \cite{CrunchyDocHA}
    \item \textbf{Postgres updates:} PGO is able to apply minor patches \cite{CrunchyDocMinorUpdates}, and major upgrades since version 5.1. \cite{CrunchyBlogUpdates}
    \item \textbf{Backups:} PGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob), ad hoc backups, and backup encryption. \cite{CrunchyDocBackups}
    \item \textbf{Disaster Recovery:} PGO is capable of Point In Time recovery, in place Point in Time Recovery, restore of an individual database. \cite{CrunchyDocDisasterRecovery}
    \item \textbf{Cloning:} PGO is able to clone cluster. \cite{CrunchyDocDisasterRecovery}
    \item \textbf{Monitoring:} Monitoring is provided by Prometheus, Grafana, and Alertmanager. \cite{CrunchyDocMonitoring}
    \item \textbf{Connection Pooling:} PgBouncer connection pooler from Postgres is part of PGO. \cite{CrunchyDocConnectionPooling}
    \item  \textbf{Customization:} PGO provides a wide area of Postgres customization. \cite{CrunchyDocCustomisation}
\end{itemize}


PGO consists of the following key components \cite{CrunchyPGOGit}:
\begin{itemize}
    \item High Availability: Patroni
    \item Backups: PgBackRest
    \item Connection Pooler: PgBouncer
    \item Monitoring: PgMonitor, Prometheus, Grafana, and Alertmanager
\end{itemize}
\obr{PGO’s architecture \cite{CrunchyDocArchitecture}}{}{1}{graphics/pgo_architecture.png}


CNPGO’s capabilities are the following:
\begin{itemize}
    \item \textbf{Postgres Cluster Provisioning:} CNPGO is able to create, update or delete Postgres cluster. \cite{CNPGdocuCapabilityLevels}
    \item \textbf{High Availability:} High availability is achieved by adding additional nodes. PGO uses a synchronous replication technique with Primary and Standby architecture. \cite{CNPGdocuReplication}
    \item \textbf{Direct database imports:} CNPGO provides direct database import from remote Postgres server by using pg\_dump and pg\_restore even on different Postgres versions. \cite{CNPGdocuDatabaseImports}
    \item \textbf{Postgres updates:} CNPGO is able to apply minor patches. \cite{CNPGdocuUpdates} Major updates are possible by previously mentioned Direct database imports.
    \item \textbf{Backups:} CNPGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob), on-demand backups, and backup encryption \cite{CNPGdocuBackup}\cite{CNPGdocuTDE}. Due to EDB’s backup software Barman backup compression is available also. \cite{CNPGdocuBackup}
    \item \textbf{Disaster Recovery:} CNPGO is capable of Point In Time recovery. Single database recovery was not mentioned in the documentation. \cite{CNPGdocuBackup}
    \item \textbf{Cloning:} CNPGO is able to create cluster replicas. \cite{CNPGdocuReplication}
    \item \textbf{Monitoring:} Monitoring can be provided by the additional installation of Prometheus, and Grafana, and Alertmanager. \cite{CNPGdocuQuickstart}
    \item \textbf{Connection Pooling:} Is provided by native Postgres pooler PgBouncer. \cite{CNPGdocuConnectionPooling}
    \item \textbf{Customization:} CNPGO provides a wide area of Postgres customization such as max parallel workers tuning or WAL configuration \cite{CNPGdocuConfiguration}
\end{itemize}


CNPGO consists of the following key components \cite{PostgresOnKubernetes} \cite{CNPGdocuQuickstart}:
\begin{itemize}
    \item High Availability: Postgres instance manager
    \item Backups: Barman
    \item Connection Pooler: PgBouncer
    \item Monitoring: Prometheus, Grafana, and Alertmanager
\end{itemize}

\obr{CNPGO’s architecture \cite{CNPGdocuConnectionPooling}}{}{1}{graphics/CNPGO_architecture.png}

SPGO’s capabilities are the following \cite{OnGres}:
\begin{itemize}
    \item \textbf{Postgres Cluster Provisioning:} SPGO is able to create, update or delete Postgres cluster.
    \item \textbf{High Availability:} High availability is achieved by adding additional nodes with Primary and Standby architecture.
    \item \textbf{Postgres updates:} SPGO is able to apply minor patches. Major updates are possible by SGDbOps \cite{SPGODocuMajorUpdates}.
    \item \textbf{Backups:} SPGO backup capabilities features: automatic backup schedules, backup to multiple locations, backup to cloud providers (AWS S3, Google Cloud Storage, Azure Blob)
    \item \textbf{Disaster Recovery:} SPGO is capable of Point In Time recovery. Database recovery was not mentioned in the documentation.
    \item \textbf{Cloning:} SPGO is able to create cluster replicas.
    \item \textbf{Monitoring:} Monitoring is provided by Prometheus, Grafana, and Alertmanager.
    \item \textbf{Connection Pooling:} Is provided by native Postgres pooler PgBouncer.
    \item \textbf{Customization:} SPGO provides a wide area of Postgres customization such as WAL configuration, archive mode, vacuum, etc. \cite{SPGODocuCustomization}
    \item \textbf{Mamagement Console:} SPGO provides a fully featured management web console.

\end{itemize}

SPGO consists of the following key components \cite{PostgresOnKubernetes}:
\begin{itemize}
    \item High Availability: Patroni
    \item Backups: WAL-G
    \item Connection Pooler: PgBouncer
    \item Monitoring: Prometheus, Grafana, and Alertmanager.
\end{itemize}


\obr{SPGO’s architecture \cite{SPGOdocuArchitecture}}{}{1}{graphics/SPGO_architecture.png}

Differences between PGO and PPO are the following:
\begin{itemize}
    \item \textbf{Postgres updates:} PPO provides automatic Postgres updates for minor and major versions of Postgres.  \cite{PerconaDocuUpdate}
    \item \textbf{Backups:} PPO is not able to back up to Azure. \cite{PerconaDocuCompare} Although it uses Patroni, which has this ability.
    \item \textbf{Disaster Recovery:} PPO documentation does not mention the possibility of restoring a single database from a backup. \cite{PerconaDocuBackups}
    \item \textbf{Monitoring:} PPO is not using the usual monitoring stack consisting of Prometheus and Grafana but their own Percona Monitoring and Management. \cite{PerconaDocuMonitoring}
\end{itemize}

\obr{PPO’s architecture \cite{CrunchyDocArchitecture}}{}{1}{graphics/PPO_architecture.png}

\n{2}{Key Differences between selected Operators}

\tab{Table of key differences between selected Operators}{tab:keyDifferences}{1}{|l|l|l|l|l|}{
    \hline
    & PGO 5.3.0 & CNPGO 1.18.1 & SPGO 1.4.2 & PPO 1.3.0\\ \hline
    \vtop{\hbox{\strut Programming}\hbox{\strut language}} & GO & GO & JAVA & GO\\ \hline
    \vtop{\hbox{\strut Supported}\hbox{\strut Postgres}}  & 12, 13, 14, 15 & 11, 12, 13, 14, 15 & 12, 13, 14, 15 & 12, 13, 14\\ \hline
    \vtop{\hbox{\strut Major version}\hbox{\strut updates}} & Manual & Manual & Manual & Automatic\\ \hline
    \vtop{\hbox{\strut Single database}\hbox{\strut restore}} & Yes & No & No & No\\ \hline
    Licence & Apache 2.0 & Apache 2.0 & Apache 2.0 & AGPL 3\\ \hline
    User interface &  No & No & Yes & No\\ \hline
    Capability Level & Autopilot & Autopilot & Deep Insight & Deep Insights\\ \hline
}

PPO consists of the following key components \cite{PostgresOnKubernetes}:
\begin{itemize}
    \item High Availability: Patroni
    \item Backups: PgBackRest
    \item Connection Pooler: PgBouncer
    \item Monitoring: Percona Monitoring and Management
\end{itemize}


SMAZAT TO CO JE POD TIMTO



Therefore, the test process derived from this methodology as every test process will not exhaustively test the Operators and will be depended on thesis context and author bias. Because the goal of this thesis is the comparison of the selected Postgres Operators for lifecycle management the main scope of this methodology is to deliver test process that will produce test reports that will form the base for this comparison. The methodology for test process will describe this test process from high level to lower levels.

\n{2}{Requirements}
As our the thesis subject is the management of lifecycle events by the Operator, testing will be focusing on testing of all the lifecycle events described in chapter~\ref{chap:lifecycle}, with an emphasis on automation.

These life cycle events can be break down into the following requirements:

\begin{requirements}
    \item The Operator must be able to create Postgres cluster.
    \item The Operator must be able to upgrade Postgres cluster to new major version.
    \item The Operator must be able to upgrade Postgres cluster to new minor version.
    \item The Operator must be able to backup cluster.
    \item The Operator must be able to perform failover.
    \item The Operator must be able to perform switchover.
    \item The Operator must be able to restore cluster from backup.
    \item The Operator must be able to scale cluster vertically.
    \item The Operator must be able to scale cluster horizontally.
    \item The Operator must be able to update cluster configuration.
    \item The Operator must be able to uninstall cluster.
    \item The Operator must provide real time performance monitoring.
    \item The Operator must provide real time resource usage monitoring.
    \item The Operator must provide real time health monitoring.
    \item The Operator should be secure.
    \item The Operator should provide comprehensive documentation.
    \item The Operator should be well-maintained.
    \item The Operator should have a robust community.
\end{requirements}

\n{2}{Test management}

\n{2}{Test management process}





\n{3}{Test strategy}
\begin{itemize}
    \item Test purpose: The purpose of testing is to evaluate the ability of Operators to fulfill the desired requirements, and to provide information for making informed decisions on which Operator to select in last chapter.
    \item Test item: test item for the test process will be Operators, their documentation, and repositories.
          \begin{itemize}
              \item PGO
                    \begin{itemize}
                        \item	Operator - Crunchy Postgres for Kubernetes Operator version 5.3.1
                        \item	Repository -  https://github.com/CrunchyData/postgres-operator
                        \item	Documentation - https://access.crunchydata.com/documentation/postgres-operator/v5/
                    \end{itemize}
              \item CNPGO
                    \begin{itemize}
                        \item Operator – CloudNativePG Operator version 1.19.1
                        \item Repository -  https://github.com/cloudnative-pg/cloudnative-pg
                        \item Documentation - https://cloudnative-pg.io/documentation/1.19/
                    \end{itemize}

              \item SGO
                    \begin{itemize}
                        \item Operator – StackGres Operator version 1.4
                        \item Repository -  https://gitlab.com/ongresinc/stackgres
                        \item Documentation - https://stackgres.io/doc/1.4/
                    \end{itemize}

              \item PPO
                    \begin{itemize}
                        \item Operator – Percona Operator for PostgreSQL version 1.4
                        \item Repository -  https://github.com/percona/percona-postgresql-operator
                        \item Documentation - https://docs.percona.com/percona-operator-for-postgresql/index.html
                    \end{itemize}
          \end{itemize}

    \item Test types
          \begin{itemize}
              \item Testing processes will utilize functional testing and non-functional testing.
          \end{itemize}

    \item Test environments
          \begin{itemize}
              \item Kind Kubernetes cluster with two worker nodes for all dynamic test except performance tests, installed on Unix/Linux compatible machine.
          \end{itemize}

    \item Roles required
          \begin{itemize}
              \item Test architect, test manager, test designer, test automator, tester and test analyst
          \end{itemize}

    \item Staff
          \begin{itemize}
              \item Miroslav Šiřina
          \end{itemize}

    \item Training needed
          \begin{itemize}
              \item	Test management
              \item	Test design
              \item	Test analyst
              \item	KUTTL
              \item	Trivy and results interpretation skills
          \end{itemize}

    \item Test risks
          \begin{itemize}
              \item Limited staff and time might prevent thorough testing of all features and functionalities of the software during acceptance testing.
              \item	Inadequately trained staff might struggle to design effective test cases, which could result in missed defects and lower overall testing effectiveness.
              \item	Due to the lack of expertise among staff members, the software's readiness for production might be inaccurately assessed, leading to incorrect conclusions about its quality and suitability for release.
          \end{itemize}
    \item Test priorities
          \begin{itemize}
              \item Static tests have higher priority to dynamic
              \item Critical features have higher priority
          \end{itemize}

    \item Test complete criteria
          \begin{itemize}
              \item 100\% coverage of all requirements
          \end{itemize}

    \item Collected metrics
          \begin{itemize}
              \item TBD
          \end{itemize}
\end{itemize}




\n{2}{Static Tests Management Process}

\n{2}{Dynamic Tests Management Process}


Fist the overal test plan must be created together with strategies



\n{2}{Test plans}
In order to completely cover all requirements, testing will be carried out according to the following test plans. First, a static review of the documentation will be performed to check if the operators cover the proposed requirements at all, then dynamic black box tests will be performed using use cases to check if the documentation does not differ from the actual state.


\tab{Test plan No 1}{tab:testplan1}{1}{|l|l|}{
    \hline
    Test plan ID & 1 \\ \hline
    Revision & 1 \\ \hline
    Introducton &  \makecell{The initial test plan to recognize if the Operator \\ can handle the basic requirements and to determine \\ the level of automation on these requirements. } \\ \hline
    Test items & 	Operator documentation \\ \hline
    Requirements to be covered & \makecell{ RQ1, RQ2, RQ3, RQ4, RQ5, RQ6, RQ7, \\ RQ8, RQ9, RQ10, RQ11, RQ12, RQ13, RQ14 } \\ \hline
    Test type &	Static \\ \hline
    Test approach &	Checlist based documentation review \\ \hline
    Entry criteria & Created checklist \\ \hline
    Exit criteria & 100\% checklist coverage \\ \hline
    Delivarables & Completed checklist for each Operator \\ \hline
    Duration & 2 h for each Operator \\ \hline
    Reviewer & Miroslav Šiřina \\ \hline
    Start & TBD \\ \hline
}

\tab{Test plan No 2}{tab:testplan2}{1}{|l|l|}{
    \hline
    Test plan ID & 2 \\ \hline
    Revision & 1 \\ \hline
    Introducton & \makecell{ This test plan is to plan the dynamic test \\ on gathered requirements and to test that \\ the Operator can deliver what was promised \\ in the documentation. } \\ \hline
    Test items & 	Operator \\ \hline
    Requirements to be covered & \makecell{ RQ1, RQ2, RQ3, RQ4, RQ5, RQ6, RQ7, \\ RQ8, RQ9, RQ10, RQ11, RQ12, RQ13, RQ14 } \\ \hline
    Test type &	Dynamic \\ \hline
    Test approach &	Black box - use cases\\ \hline
    Test envrioment & Kind kubernetes cluster \\ \hline
    Test tools & Kubectl, KUTTL \\ \hline
    Entry criteria & \makecell{  Checklist from previous static testing. \\ Use cases.}\\ \hline
    Exit criteria & 100\% use case coverage \\ \hline
    Delivarables &	Test report \\ \hline
    Duration & 8 h for each Operator \\ \hline
    Automation & All test except monitoring tests must be automated. \\ \hline
    Tester & Miroslav Šiřina \\ \hline
    Start & TBD \\ \hline
}


\tab{Test plan No 3}{tab:testplan3}{1}{|l|l|}{
    \hline
    Test plan ID & 3 \\ \hline
    Revision & 1 \\ \hline
    Introducton &  \makecell{ Vulnerability analysis test plan  } \\ \hline
    Test items & 	Operator \\ \hline
    Requirements to be covered & \makecell{ RQ15 } \\ \hline
    Test type &	Static \\ \hline
    Test approach &	Static analysis \\ \hline
    Test envrioment & Kind kubernetes cluster \\ \hline
    Test tools & Trivy \\ \hline
    Entry criteria & \makecell{ Deployed operator }\\ \hline
    Exit criteria & Vulnerability analysis finished for each Operator\\ \hline
    Delivarables &	Test report \\ \hline
    Duration & 2 h for each Operator \\ \hline
    Tester & Miroslav Šiřina \\ \hline
    Start & TBD \\ \hline
}+

\tab{Test plan No 4}{tab:testplan4}{1}{|l|l|}{
    \hline
    Test plan ID & 4 \\ \hline
    Revision & 1 \\ \hline
    Introducton &  \makecell{ Gathering metrics from Operator repositories \\ in order to realise how well maintained \\ the Operator and the size of the \\ Operator community. } \\ \hline
    Test items & 	Operator documentation \\ \hline
    Requirements to be covered & RQ17, RQ18 \\ \hline
    Test type &	Static \\ \hline
    Test approach &	Repository review \\ \hline
    Entry criteria & List of metrics to be collected \\ \hline
    Exit criteria & Metrics collected, ratio between Operators \\ \hline
    Delivarables & Test report \\ \hline
    Duration & 2 h for each Operator \\ \hline
    Reviewer & Miroslav Šiřina \\ \hline
    Start & TBD \\ \hline
}


\tab{Test plan No 5}{tab:testplan5}{1}{|l|l|}{
    \hline
    Test plan ID & 5 \\ \hline
    Revision & 1 \\ \hline
    Introducton &  \makecell{ if the Operator \\ can handle the basic requirements and to determine \\ the level of automation on these requirements. } \\ \hline
    Test items & 	Operator documentation \\ \hline
    Requirements to be covered & RQ16 \\ \hline
    Test type &	Static \\ \hline
    Test approach &	Checlist based documentation review \\ \hline
    Entry criteria & Created checklist \\ \hline
    Exit criteria & 100\% checklist coverage \\ \hline
    Delivarables &	Completed checklist for each Operator \\ \hline
    Duration & 2 h for each Operator \\ \hline
    Reviewer & Miroslav Šiřina \\ \hline
    Start & TBD \\ \hline
}


\n{2}{Test designs}
\n{3}{Test plan No 1}













\n{1}{Testing methodology - before I knew what testing is}
As mentioned in the previous chapters, Operators will be subjected to tests to manage the complete application lifecycle.

Subsequently:
\begin{itemize}
    \item Operator install
          \begin{itemize}
              \item Installation of the Operator in Kubernetes cluster
              \item Verify that the Operator was installed
          \end{itemize}
    \item Cluster install
          \begin{itemize}
              \item Installation of the cluster with an older version of Postgres and two nodes, one as Primary, the other as Standby
              \item Verify that all components were installed successfully
          \end{itemize}
    \item Cluster accepts connections
          \begin{itemize}
              \item Connection to cluster
              \item Verify that connection was successful
          \end{itemize}
    \item Cluster synchronization
          \begin{itemize}
              \item Creation of database on primary node
              \item Verify that database created in previous step was also created on Standby node
          \end{itemize}
    \item Cluster update to a newer version
          \begin{itemize}
              \item Update the cluster to a newer version
              \item Verify update result
          \end{itemize}
    \item Recovery of the Primary node after a crash
          \begin{itemize}
              \item Delete of the primary node
              \item Failover verification
              \item New Standby creation verification
          \end{itemize}
    \item Cluster backup
          \begin{itemize}
              \item Creation of backup volume
              \item Postgres cluster backup to backup volume
              \item Backup result verification
          \end{itemize}
    \item Cluster restore from backup
          \begin{itemize}
              \item Restoration of the cluster from the backup created in the previous step
              \item Restored cluster verification
          \end{itemize}
    \item Cluster vertical scaling
          \begin{itemize}
              \item Addition of a node to the cluster
              \item Number of nodes verification
              \item Removal of the added node
              \item Number of nodes verification
          \end{itemize}
    \item Cluster configuration update
          \begin{itemize}
              \item Change the cluster configuration
              \item TBD - figure out what we are going to change and justify it
              \item Verifify configuration changes
          \end{itemize}
    \item Cluster monitoring
          \begin{itemize}
              \item Installation of monitoring
              \item Visual monitoring verification
          \end{itemize}
    \item Cluster uninstall
          \begin{itemize}
              \item Removal of the backup volume
              \item Removal of the cluster
              \item Verify cluster removal
          \end{itemize}
    \item Operator uninstall
          \begin{itemize}
              \item Operator uninstall
              \item Verify that Operator was removed with all its dependencies
          \end{itemize}
\end{itemize}


TBD: study what the methodology is, figure out how to rate each point

TBD: Describe architecture

TBD: Describe kind cluster (maybe all in theory)

TBD: Describe kubectl

TBD: First create manual testing and describe it, then automate these test with Kubernetes E2E tests in GO

The repository of this thesis contains a folder of tests in which the individual test suites are located. Each operator has a folder (test suite) that contains individual test cases numbered according to the order of execution, followed by a Makefile file that lists the specific commands that are executed to test the Operator.

TBD: describe isntalled dependencies: kubectl, psql, jq, busybox

\pagebreak
\n{2}{Crunchy Postgres for Kubernetes (PGO) functional tests}
The Crunchy documentation includes a Quickstart chapter\footnote{https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart} that describes examples of Operator installation and usage. This chapter also contains a link to a repository with yaml manifests\footnote{https://github.com/CrunchyData/postgres-operator-examples} that allow kubectl to install, modify or delete Operator and managed resources in a kubernetes cluster. The manifests are distributed under the Apache 2.0 license and are also part of the repository of this thesis\footnote{https://github.com/Ovec/Bachelors-thesis}. In case of a any modification of these manifests, the reason for the modification and the modification itself is described.



\n{3}{Operator install}
The installation of the operator was done with the following commands in install directory\footnote{/tests/PGO/1\_operator\_install}, that contains yaml manifests extracted from the PGO examples repository without any change.

\textcolor{cyan}{this will install pgo version 5.3.1}

\begin{verbatim}
kubectl apply -k ./namespace
kubectl apply --server-side -k ./default
\end{verbatim}

These commands created the following kubernetes objects:
\begin{itemize}
    \item Operator's Custom resource definition named \\postgresclusters.postgres-operator.crunchydata.com
    \item Operators service accounts named pgo and postgres-operator-upgrade
    \item Cluster roles postgres-operator
    \item Cluster role postgres-operator-update
    \item A cluster role binding named postgres-operator that links service account pgo to the cluster role postgres-operator
    \item Cluster role binding named postgres-operator-upgrade which links service account postgres-operator-upgrade to cluster role postgres-operator-upgrade
    \item Deployment pgo
    \item Deployment pgo-upgrade
\end{itemize}


To verify that the Operator is created correctly, the following command was used:
\begin{verbatim}
kubectl -n postgres-operator get pods
\end{verbatim}

This command displays all pods from the postgres-operator namespace. If the Operator was installed correctly the command will display the pod operator with the status Running.

The result of this command was the following:
\begin{verbatim}
NAME                           READY   STATUS    RESTARTS
pgo-864ddbd4cf-4pcd6           1/1     Running   0
pgo-upgrade-78fc8d8f94-jbnrl   1/1     Running   0 
\end{verbatim}

Therefore, it can be stated that the Operator has been installed and has successfully passed this test case.

In order to run this test case again following basch script was created and stored in Operator installation folder. If the script exits with 0 the test was passed successfully.

\begin{verbatim}
#!/bin/bash
source ../../utilities/pods/wait_for_all_pods_in_namespace_running.sh

kubectl apply -k ./namespace

kubectl apply --server-side -k ./default

sleep 5
wait_for_all_pods_in_namespace_running "postgres-operator"

echo "Success"
exit 0
\end{verbatim}


\n{3}{Cluster install with older major Postgres version}
\label{chap:pgoclusterinstall}
By using the kubectl describe command on the operator pod, the supported Postgres docker image, pgAdmin, pgBackRest, pgBouncer and pgExporter can be displayed. The result for the deployed Operator was the following\footnote{'registry.developers.crunchydata.com/crunchydata/' is being replaced with '<registry>'}:

\begin{verbatim}
PGO_NAMESPACE:                      
postgres-operator (v1:metadata.namespace)                                    
CRUNCHY_DEBUG:                      
true                                  
RELATED_IMAGE_POSTGRES_13:          
<registry>crunchy-postgres:ubi8-13.10-0
RELATED_IMAGE_POSTGRES_13_GIS_3.0:  
<registry>crunchy-postgres-gis:ubi8-13.10-3.0-0                               
RELATED_IMAGE_POSTGRES_13_GIS_3.1:  
<registry>crunchy-postgres-gis:ubi8-13.10-3.1-0                               
RELATED_IMAGE_POSTGRES_14:          
<registry>crunchy-postgres:ubi8-14.7-0 
RELATED_IMAGE_POSTGRES_14_GIS_3.1:  
<registry>crunchy-postgres-gis:ubi8-14.7-3.1-0                                
RELATED_IMAGE_POSTGRES_14_GIS_3.2:  
<registry>crunchy-postgres-gis:ubi8-14.7-3.2-0                                
RELATED_IMAGE_POSTGRES_14_GIS_3.3:  
<registry>crunchy-postgres-gis:ubi8-14.7-3.3-0                                
RELATED_IMAGE_POSTGRES_15:          
<registry>crunchy-postgres:ubi8-15.2-0 
RELATED_IMAGE_POSTGRES_15_GIS_3.3:  
<registry>crunchy-postgres-gis:ubi8-15.2-3.3-0                                
RELATED_IMAGE_PGADMIN:              
<registry>crunchy-pgadmin4:ubi8-4.30-10
RELATED_IMAGE_PGBACKREST:           
<registry>crunchy-pgbackrest:ubi8-2.41-4                                      
RELATED_IMAGE_PGBOUNCER:            
<registry>crunchy-pgbouncer:ubi8-1.18-0
RELATED_IMAGE_PGEXPORTER:           
<registry>crunchy-postgres-exporter:ubi8-5.3.1-0
\end{verbatim}

Since the yaml manifest provided contains Postgres version 14, but the list of supported versions includes version 15, the manifests will be used as is and Postgres will be updated from version 14 to version 15 in the Postgres version update test case (Chapter ~\ref{chap:pgosync}).

To install the Postgres cluster, the following command must be executed in the install directory\footnote{/tests/PGO/1\_cluster\_install}.
\begin{verbatim}
kubectl apply -k ./high-availability
\end{verbatim}

This command has installed the following pods and their services in postgres-operator namespace

\begin{verbatim}
NAME                                  READY   STATUS      RESTARTS
hippo-ha-backup-2bc7-rv8s9            0/1     Completed   0
hippo-ha-pgbouncer-7c6fcd4668-9fn7v   2/2     Running     0
hippo-ha-pgbouncer-7c6fcd4668-jsvmj   2/2     Running     0
hippo-ha-pgha1-nmq4-0                 4/4     Running     0
hippo-ha-pgha1-pmsk-0                 4/4     Running     0
hippo-ha-repo-host-0                  2/2     Running     0
\end{verbatim}

\begin{verbatim}
NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)
hippo-ha-ha          ClusterIP   10.96.225.2    <none>        5432/TCP
hippo-ha-ha-config   ClusterIP   None           <none>        <none>
hippo-ha-pgbouncer   ClusterIP   10.96.49.246   <none>        5432/TCP
hippo-ha-pods        ClusterIP   None           <none>        <none>
hippo-ha-primary     ClusterIP   None           <none>        5432/TCP
hippo-ha-replicas    ClusterIP   10.96.74.192   <none>        5432/TCP
\end{verbatim}

Using kubectl describe, it was found that the hippo-ha-pgha pods contain four containers including a Postgres server, a pgBackrest server for backups, Patroni for replica synchronization, and replication-cert. Hippo-ha-pgbouncer pods contain pgBouncer and pgBouncer-config containers taking care of connection pooling for hippo-ha-pgha pods. Hippo-ha-repo-host-0 contains the pgBackrest server and pgBackrest-config containers from the yaml manifest it was found that it created a volume and backed up the database server when it was created.

Using kubectl describe, it was found that the service hippo-ha-ha is connected to the pod hippo-ha-pgha1-pmsk-0, the service hippo-ha-pgbouncer is connected to the pod pgBouncer, the service hippo-ha-primary is connected to the service hippo-ha-ha, and hippo-ha-replicas is connected to the pod hippo-ha-pgha1-nmq4-0. It can thus be concluded that there should be a primary replica under hippo-ha-pgha1-pmsk-0 and a standby replica under hippo-ha-pgha1-nmq4-0. This finding will be verified in Chapter ~\ref{chap:pgosync}

All components were successfully installed and Kubernetes did not report any errors so it can be said that Operator passed this test successfully.

In order to run this test case again check.sh script can be used in cluster installation folder. If the script exits with 0 the test was passed successfully. The content of the script is as follows:
\begin{verbatim}

\end{verbatim}

\n{3}{Cluster accepts connections}
Since the Postgres cluster is not exposed outside the cluster, it was necessary to install some utility in the cluster that will have telnet which can connect to port 5432 of the cluster, like BusyBox.

Yaml manifest of the installed BusyBox.
\begin{verbatim}
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    - name: busybox
      image: busybox
      command: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
\end{verbatim}

Busybox was installed with the following command executed in test directory\footnote{/tests/PGO/3\_cluster\_accepts\_connections}.
\begin{verbatim}
kubectl apply -f ./busybox.yaml
\end{verbatim}

The connection to the cluster was successfully established with the following command:

\begin{verbatim}
kubectl exec busybox -- telnet \
$(kubectl get service hippo-ha-pgbouncer -n postgres-operator \
-o jsonpath='{.spec.clusterIP}') 5432
\end{verbatim}

Therefore, it can be said that the cluster is receiving the connection and has passed this test successfully.

\n{3}{Cluster synchronization}
\label{chap:pgosync}
In order to find out if the nodes are synchronized, a test database will be created on the Primary node and then the name of this database will be selected from the list of databases on Standby node.

This was achieved by the following commands. The first command created the database on the primary node and the second checked for its existence on the standy. The commands contain subcommands that return the name of the primary or standby node depending on the set selector.

\begin{verbatim}
kubectl exec $(kubectl -n postgres-operator get pods \
--selector=postgres-operator.crunchydata.com/role=master \
-o jsonpath='{.items[*].metadata.name}') \
-n postgres-operator \
-- psql -U postgres -c "create database test"
\end{verbatim}
\begin{verbatim}
kubectl exec $(kubectl -n postgres-operator get pods \
--selector=postgres-operator.crunchydata.com/role=replica \
-o jsonpath='{.items[*].metadata.name}') \
-n postgres-operator \
-- psql -U postgres -c "SELECT 1 as success FROM 
pg_database WHERE datname = 'test'"
\end{verbatim}

\n{3}{Cluster update to a newer version}
\label{chap:pgoupdate}
Updating a major version of the Postgres database server using the PGO Operator consists of the following steps. The first step must be installed using kubectl pgUpgrade which manifest is depicted below.
\begin{verbatim}
kubectl apply -k ./pg-upgrade
\end{verbatim}

Additionally, the allow update annotation must be added to the hippo-ha cluster using the following command.
\begin{verbatim}
kubectl -n postgres-operator annotate postgrescluster hippo-ha \
postgres-operator.crunchydata.com/allow-upgrade="hippo-upgrade"
\end{verbatim}


Then the cluster must be shut down by adding the spec parameter shutdown set to true in the same yaml manifest under which the cluster was created and then applied using the same command as the cluster was created.

Using the command that lists the status of pgUpgrade\footnote{kubectl describe pgupgrade hippo-upgrade -n postgres-operator}, upgrade status can be monitored. The condition section lists the status of the upgrade, it looks similar to the one below.

\begin{verbatim}
Last Transition Time:  2023-03-24T16:40:13Z
Message:               Upgrade progressing for cluster hippo-ha
Observed Generation:   1
Reason:                PGUpgradeProgressing
Status:                True
Type:                  Progressing
\end{verbatim}
\begin{verbatim}
Last Transition Time:  2023-03-24T16:46:39Z
Message:               PostgresCluster hippo-ha is running version 15
Observed Generation:   1
Reason:                PGUpgradeCompleted
Status:                False
Type:                  Progressing
\end{verbatim}
\begin{verbatim}
Last Transition Time:  2023-03-24T16:46:39Z
Message:               PostgresCluster hippo-ha is ready 
                       to complete upgrade to version 15
Observed Generation:   1
Reason:                PGUpgradeSucceeded
Status:                True
Type:                  Succeeded
\end{verbatim}

Once the conditions status is Succeeded, the upgrade is complete. Then the manifest still needs to be changed again, setting the spec shutdown parameter to false, the postgresVersion parameter to 15 and the container image to the supported version shown above in Chapter~\ref{chap:pgoclusterinstall}.

The cluster was successfully upgraded from version 14.7 to version 15.2 by this process and contained a test database. Therefore, Operator passed this test successfully.

\n{3}{Recovery of the Primary node after a crash}
The failover was verified as follows. The names of the primary and standby nodes were stored in variables. Then the primary node was deleted. A check was made to see if the name of the primary node was the same as the name of the node stored in the variable, followed by a check to see if the name of the new standby node was the same as the name of the original primary node. The following script was used for the verfiy it.

\begin{verbatim}
#!/bin/bash
source ../../utilities/pods/get_podname_by_selector.sh

primary=$(get_pod_name_by_selector \
    postgres-operator.crunchydata.com/role master)
standby=$(
    get_pod_name_by_selector \
        postgres-operator.crunchydata.com/role replica
)
kubectl delete pod "$primary" -n \
    postgres-operator
new_primary=$(
    get_pod_name_by_selector \
        postgres-operator.crunchydata.com/role master
)

counter=0
new_standby=""

while [[ "$counter" -lt 30 ]]; do
    new_standby=$(
        get_pod_name_by_selector \
            postgres-operator.crunchydata.com/role replica
    )
    if test "$new_standby" = ""; then
        sleep 1
    else
        break
    fi
done

if test "$standby" = "$new_primary" &&
    test "$primary" = "$new_standby"; then
    exit 0
else
    echo "Error: failover failed"
    exit 1
fi
\end{verbatim}

The shell script was executed without error and therefore the Operator passed the test.


\n{3}{Cluster backup}
PGO enables automatic backup. The cluster installation manifest has this backup set up. Therefore the cluster was backed up when it was created and after it was upgraded to a newer version, to check if it was backed up just check the existence of backups in the /pgbackrest/repo1/archive/db/15-2/ pgBackRest pod directory. The verification was done with the following script.

\begin{verbatim}
#!/bin/bash
source ../../utilities/pods/get_podname_by_selector.sh

cmd="kubectl exec $(get_pod_name_by_selector \
    postgres-operator.crunchydata.com/data pgbackrest) \
    -n postgres-operator \
    -- ls /pgbackrest/repo1/archive/db/15-2/"

if $cmd && [ -n "$cmd" ]; then
    exit 0
else
    echo "Backup directory doesn't exist or its empty"
    exit 1
fi
\end{verbatim}
The shell script was executed without error and therefore the Operator passed the test.

\n{3}{Cluster restore from backup}
\n{3}{Cluster vertical scaling}
\n{3}{Cluster configuration update}
\n{3}{Cluster monitoring}
- monitoring pgo provides monitoring but you have to install it

- you must also add PGO exporter in order to export metrics to Prometheus

- Pgo provides five Grafana dashboards. POD details fig. \ref{fig:PGO_monitoring_POD_details} (various pod statistics for each pod, CPU, memory, network and disk usage for each POD),
PostgreSQL service health fig. \ref{fig:PGO_monitoring_postgresSQL_health} (Saturation, Traffic, Errors, Query durration),
PgBackRest dashboard fig. \ref{fig:PGO_monitoring_pgBackRest} (Information about backups),
PostgreSQL details \ref{fig:PGO_monitoring_postgresSQLdetails} (complete cluster overview including active conncetions, cache hit ratio, database size, etc.),
and Query statistics \ref{fig:PGO_monitoring_QueryStatistics} (verry helpfull statistic for each Postgres operator, shows query mean time, query run time, and highlites slow queries)

- After installing Prometheus exporter everything worked flawlessly, cluster was discovered by Prometheus and appeared in Grafana

\obr{PGO monitoring POD details}{fig:PGO_monitoring_POD_details}{1}{graphics/PGO_monitoring_POD_details.png}
\obr{PGO monitoring PostgreSQL health}{fig:PGO_monitoring_postgresSQL_health}{1}{graphics/PGO_monitoring_postgresSQL_health.png}
\obr{PGO monitoring PgBackRest}{fig:PGO_monitoring_pgBackRest}{1}{graphics/PGO_monitoring_pgBackRest.png}
\obr{PGO monitoring PostgreSQL details}{fig:PGO_monitoring_postgresSQLdetails}{1}{graphics/PGO_monitoring_postgresSQLdetails.png}
\obr{PGO monitoring Query Statistics}{fig:PGO_monitoring_QueryStatistics}{1}{graphics/PGO_monitoring_QueryStatistics.png}


\n{3}{Cluster uninstall}



\n{3}{Operator uninstall}

